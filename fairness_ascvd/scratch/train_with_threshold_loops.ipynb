{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "import copy\n",
    "import configargparse as argparse\n",
    "\n",
    "from prediction_utils.util import yaml_write\n",
    "from prediction_utils.pytorch_utils.models import TorchModel\n",
    "from prediction_utils.pytorch_utils.lagrangian import MultiLagrangianThresholdRateModel\n",
    "from prediction_utils.pytorch_utils.robustness import GroupDROModel\n",
    "\n",
    "from prediction_utils.pytorch_utils.metrics import StandardEvaluator, FairOVAEvaluator, CalibrationEvaluator\n",
    "\n",
    "import git\n",
    "repo = git.Repo('.', search_parent_directories=True)\n",
    "os.chdir(repo.working_dir) \n",
    "\n",
    "import train_utils\n",
    "import yaml\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--experiment_name', type=str)\n",
    "# parser.add_argument('--cohort_path', type=str) \n",
    "# parser.add_argument('--result_path', type=str)\n",
    "# parser.add_argument('--logging_path', type=str)\n",
    "# # parser.add_argument('--base_path', type=str)\n",
    "# parser.add_argument('--config_id', type=str)\n",
    "# parser.add_argument('--fold_id', type=str)\n",
    "# parser.add_argument('--print_debug', type=bool)\n",
    "# parser.add_argument('--save_outputs', type=bool)\n",
    "# parser.add_argument('--run_evaluation', type=bool)\n",
    "# parser.add_argument('--run_evaluation_group_standard', type=bool)\n",
    "# parser.add_argument('--run_evaluation_group_fair_ova', type=bool)\n",
    "# parser.add_argument('--save_model_weights', type=bool)\n",
    "# parser.add_argument('--data_query', type=str)\n",
    "# parser.add_argument('--base_config_path', type=str)\n",
    "# parser.add_argument('--config_path', type=str)\n",
    "# parser.add_argument('--num_epochs', type=int)\n",
    "\n",
    "# parser.set_defaults(\n",
    "#     save_outputs=False,\n",
    "#     run_evaluation=True,\n",
    "#     run_evaluation_group_standard=True,\n",
    "#     run_evaluation_group_fair_ova=True,\n",
    "#     print_debug=True,\n",
    "#     save_model_weights=False,\n",
    "#     data_query = '',\n",
    "#     num_epochs = 0\n",
    "# )\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# args = copy.deepcopy(args.__dict__)\n",
    "\n",
    "\n",
    "def run_model(args, config_dict):\n",
    "    \n",
    "    ##### INITIAL SETUP #####\n",
    "    os.makedirs(args['result_path'], exist_ok=True)\n",
    "\n",
    "    logger = train_utils.logger_setup(config_dict, args)\n",
    "\n",
    "    ##### DATASET #####\n",
    "    data_df = pd.read_csv(args['cohort_path'])\n",
    "\n",
    "    if (len(args['data_query']) > 0):\n",
    "        data_df = (data_df\n",
    "                   .query(args['data_query'])\n",
    "                   .reset_index(drop=True)\n",
    "                  )\n",
    "\n",
    "    data_args = train_utils.get_dict_subset(config_dict, ['feature_columns', 'val_fold_id', 'test_fold_id', 'batch_size'])\n",
    "    data = train_utils.Dataset(data_df, deg=2, **data_args)\n",
    "\n",
    "    # add input dim to dictionary\n",
    "    config_dict.update({'input_dim': data.features_dict_uncensored_scaled['train'].shape[1]})\n",
    "\n",
    "    # log\n",
    "    logger.info(\"Result path: {}\".format(args['result_path']))\n",
    "\n",
    "    model, logger = train_utils.model_setup(config_dict, logger, args)\n",
    "\n",
    "    result_df = model.train(loaders=data.loaders_dict)['performance']\n",
    "\n",
    "    result_df.to_parquet(os.path.join(args['result_path'], \"result_df_training.parquet\"), index=False, engine=\"pyarrow\")\n",
    "\n",
    "    if args['save_model_weights']:\n",
    "        torch.save(model.model.state_dict(), os.path.join(args['result_path'], \"state_dict.pt\"))\n",
    "\n",
    "    if args['run_evaluation']:\n",
    "        logger.info(\"Evaluating model\")\n",
    "\n",
    "        predict_dict = model.predict(data.loaders_dict_predict, \n",
    "                                     phases=['val', 'test'])\n",
    "\n",
    "        # general evaluation\n",
    "        output_df_eval, result_df_eval = (\n",
    "            predict_dict[\"outputs\"],\n",
    "            predict_dict[\"performance\"]\n",
    "        )\n",
    "\n",
    "        logger.info(result_df_eval)\n",
    "\n",
    "        output_df_eval = (train_utils.add_ranges(output_df_eval)\n",
    "                          .rename(columns={'row_id': 'person_id'})\n",
    "                          .merge(data_df.filter(['person_id', 'ldlc']), how='inner', on='person_id')\n",
    "                          .assign(relative_risk = lambda x: train_utils.treat_relative_risk(x),\n",
    "                                  new_risk = lambda x: x.pred_probs*x.relative_risk\n",
    "                                 )\n",
    "                      )\n",
    "\n",
    "        # Dump evaluation result to disk\n",
    "        result_df_eval.to_parquet(\n",
    "            os.path.join(args['result_path'], \"result_df_training_eval.parquet\"),\n",
    "            index=False,\n",
    "            engine=\"pyarrow\",\n",
    "        )\n",
    "\n",
    "        if args.get('save_outputs'):\n",
    "            output_df_eval.to_parquet(\n",
    "                os.path.join(args['result_path'], \"output_df.parquet\"),\n",
    "                index=False,\n",
    "                engine=\"pyarrow\",\n",
    "            )\n",
    "\n",
    "        logger = train_utils.evaluation(output_df_eval, args, config_dict, logger)\n",
    "        \n",
    "EXPERIMENT_NAME = 'eq_oddsconstr'\n",
    "\n",
    "BASE_PATH = '/labs/shahlab/projects/agataf/data/cohorts/pooled_cohorts'\n",
    "args = {'experiment_name': EXPERIMENT_NAME,\n",
    "        'cohort_path': '/labs/shahlab/projects/agataf/data/pooled_cohorts/cohort_extraction/all_cohorts.csv',\n",
    "        'base_path': BASE_PATH,\n",
    "        'print_debug': True,\n",
    "        'save_outputs': True,\n",
    "        'run_evaluation_group_standard': True,\n",
    "        'run_evaluation_group_fair_ova': True,\n",
    "        'save_model_weights': True,\n",
    "        'run_evaluation': True,\n",
    "        'split_gender': False,\n",
    "        'data_query': ''\n",
    "       }\n",
    "\n",
    "\n",
    "BASE_CONFIG_PATH = os.path.join(BASE_PATH, 'experiments', 'basic_config.yaml')\n",
    "config_dict = yaml.load(open(BASE_CONFIG_PATH), Loader=yaml.FullLoader)\n",
    "\n",
    "update_dict = {\n",
    "    \"threshold_mode\": \"conditional\",\n",
    "    \"thresholds\": [0.075, 0.2],\n",
    "    \"surrogate_scale\": 1.0,\n",
    "    'logging_metrics': ['auc', 'auprc', 'brier', 'loss_bce'],\n",
    "    'data_query': '',\n",
    "    'group_objective_type': 'multiThreshold',\n",
    "    'evaluate_by_group': True,\n",
    "    'sparse': False,\n",
    "    'output_dim': 2,\n",
    "    \"num_groups\": 4,\n",
    "    'num_hidden': 0,\n",
    "    'weighted_loss': True,\n",
    "    'num_epochs': 10\n",
    "}\n",
    "\n",
    "config_dict.update(update_dict)\n",
    "\n",
    "configs = zip(['00', '01', '02', '03', '04', '05', '06', '07', '08', '09'],\n",
    "              np.geomspace(1e-3,1e-1,num=10))\n",
    "\n",
    "# for config_id, lambda_reg in configs:\n",
    "\n",
    "#     for fold_id in range(1,11):\n",
    "\n",
    "#         RESULT_PATH = os.path.join(args['base_path'], 'experiments', args['experiment_name'], 'performance',\n",
    "#                                    '.'.join((str(config_id), 'yaml')), str(fold_id))\n",
    "#         LOGGING_PATH = os.path.join(RESULT_PATH, 'training_log.log')\n",
    "\n",
    "#         config_dict.update({'val_fold_id': str(fold_id), \n",
    "#                             'num_epochs': 100, \n",
    "#                             'lambda_group_regularization': lambda_reg,\n",
    "#                             'logging_path': LOGGING_PATH})\n",
    "\n",
    "\n",
    "#         args.update({'result_path': RESULT_PATH})\n",
    "\n",
    "#         run_model(args, config_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 0.001\n",
      "01 0.0016681005372000592\n",
      "02 0.0027825594022071257\n",
      "03 0.004641588833612777\n",
      "04 0.007742636826811269\n",
      "05 0.01291549665014884\n",
      "06 0.021544346900318832\n",
      "07 0.03593813663804626\n",
      "08 0.05994842503189409\n",
      "09 0.1\n"
     ]
    }
   ],
   "source": [
    "for config_id, lambda_reg in configs:\n",
    "    print(config_id, lambda_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_id='scratch'\n",
    "fold_id=1\n",
    "\n",
    "RESULT_PATH = os.path.join(args['base_path'], 'experiments', args['experiment_name'], 'performance',\n",
    "                                   '.'.join((str(config_id), 'yaml')), str(fold_id))\n",
    "LOGGING_PATH = os.path.join(RESULT_PATH, 'training_log.log')\n",
    "\n",
    "config_dict.update({'val_fold_id': str(fold_id), \n",
    "                            'num_epochs': 1, \n",
    "                            'lambda_group_regularization': lambda_reg,\n",
    "                            'logging_path': LOGGING_PATH})\n",
    "\n",
    "\n",
    "args.update({'result_path': RESULT_PATH})\n",
    "\n",
    "run_model(args, config_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/labs/shahlab/projects/agataf/data/cohorts/pooled_cohorts/experiments/eq_oddsconstr/performance/scratch.yaml/1/training_log.log'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_dict.get('logging_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 128,\n",
       " 'feature_columns': ['age',\n",
       "  'totchol',\n",
       "  'hdlc',\n",
       "  'sysbp',\n",
       "  'rxsbp',\n",
       "  'unrxsbp',\n",
       "  'bmi',\n",
       "  'diabt126',\n",
       "  'cursmoke',\n",
       "  'race_black',\n",
       "  'gender_male'],\n",
       " 'group_objective_type': 'multiThreshold',\n",
       " 'logging_evaluate_by_group': True,\n",
       " 'logging_metrics': ['auc', 'auprc', 'brier', 'loss_bce'],\n",
       " 'logging_threshold_metrics': ['specificity', 'recall', 'positive_rate'],\n",
       " 'logging_thresholds': [0.075, 0.2],\n",
       " 'lr': 0.0001,\n",
       " 'metrics': ['auc',\n",
       "  'auprc',\n",
       "  'brier',\n",
       "  'loss_bce',\n",
       "  'ace_rmse_logistic_log',\n",
       "  'ace_abs_logistic_log'],\n",
       " 'num_epochs': 100,\n",
       " 'num_groups': 4,\n",
       " 'num_hidden': 0,\n",
       " 'output_dim': 2,\n",
       " 'print_every': 10,\n",
       " 'sparse': False,\n",
       " 'test_fold_id': 'test',\n",
       " 'val_fold_id': '1',\n",
       " 'weighted_evaluation': True,\n",
       " 'weighted_loss': True,\n",
       " 'threshold_mode': 'conditional',\n",
       " 'thresholds': [0.075, 0.2],\n",
       " 'surrogate_scale': 1.0,\n",
       " 'data_query': '',\n",
       " 'evaluate_by_group': True,\n",
       " 'lambda_group_regularization': 0.1,\n",
       " 'logging_path': '/labs/shahlab/projects/agataf/data/cohorts/pooled_cohorts/experiments/eq_oddsconstr/performance/scratch.yaml/1/training_log.log'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#def run_model(args, config_dict):\n",
    "    \n",
    "    ##### INITIAL SETUP #####\n",
    "    os.makedirs(args['result_path'], exist_ok=True)\n",
    "\n",
    "    logger = train_utils.logger_setup(config_dict, args)\n",
    "\n",
    "    ##### DATASET #####\n",
    "    data_df = pd.read_csv(args['cohort_path'])\n",
    "\n",
    "    if (len(args['data_query']) > 0):\n",
    "        data_df = (data_df\n",
    "                   .query(args['data_query'])\n",
    "                   .reset_index(drop=True)\n",
    "                  )\n",
    "\n",
    "    data_args = train_utils.get_dict_subset(config_dict, ['feature_columns', 'val_fold_id', 'test_fold_id', 'batch_size'])\n",
    "    data = train_utils.Dataset(data_df, deg=2, **data_args)\n",
    "\n",
    "    # add input dim to dictionary\n",
    "    config_dict.update({'input_dim': data.features_dict_uncensored_scaled['train'].shape[1]})\n",
    "\n",
    "    # log\n",
    "    logger.info(\"Result path: {}\".format(args['result_path']))\n",
    "\n",
    "    model, logger = train_utils.model_setup(config_dict, logger, args)\n",
    "\n",
    "    result_df = model.train(loaders=data.loaders_dict)['performance']\n",
    "\n",
    "    result_df.to_parquet(os.path.join(args['result_path'], \"result_df_training.parquet\"), index=False, engine=\"pyarrow\")\n",
    "\n",
    "    if args['save_model_weights']:\n",
    "        torch.save(model.model.state_dict(), os.path.join(args['result_path'], \"state_dict.pt\"))\n",
    "\n",
    "    if args['run_evaluation']:\n",
    "        logger.info(\"Evaluating model\")\n",
    "\n",
    "        predict_dict = model.predict(data.loaders_dict_predict, \n",
    "                                     phases=['val', 'test'])\n",
    "\n",
    "        # general evaluation\n",
    "        output_df_eval, result_df_eval = (\n",
    "            predict_dict[\"outputs\"],\n",
    "            predict_dict[\"performance\"]\n",
    "        )\n",
    "\n",
    "        logger.info(result_df_eval)\n",
    "\n",
    "        output_df_eval = (train_utils.add_ranges(output_df_eval)\n",
    "                          .rename(columns={'row_id': 'person_id'})\n",
    "                          .merge(data_df.filter(['person_id', 'ldlc']), how='inner', on='person_id')\n",
    "                          .assign(relative_risk = lambda x: train_utils.treat_relative_risk(x),\n",
    "                                  new_risk = lambda x: x.pred_probs*x.relative_risk\n",
    "                                 )\n",
    "                      )\n",
    "\n",
    "        # Dump evaluation result to disk\n",
    "        result_df_eval.to_parquet(\n",
    "            os.path.join(args['result_path'], \"result_df_training_eval.parquet\"),\n",
    "            index=False,\n",
    "            engine=\"pyarrow\",\n",
    "        )\n",
    "\n",
    "        if args.get('save_outputs'):\n",
    "            output_df_eval.to_parquet(\n",
    "                os.path.join(args['result_path'], \"output_df.parquet\"),\n",
    "                index=False,\n",
    "                engine=\"pyarrow\",\n",
    "            )\n",
    "\n",
    "        logger = train_utils.evaluation(output_df_eval, args, config_dict, logger)\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
