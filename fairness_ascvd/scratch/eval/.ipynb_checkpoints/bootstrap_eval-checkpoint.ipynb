{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "opposite-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "import copy\n",
    "import yaml\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "from prediction_utils.pytorch_utils.metrics import (\n",
    "    StandardEvaluator,\n",
    "    FairOVAEvaluator,\n",
    "    CalibrationEvaluator\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "grp_label_dict = {1: 'Black women', 2: 'White women', 3: 'Black men', 4: 'White men'} \n",
    "args = {'cohort_path': '/labs/shahlab/projects/agataf/data/cohorts/pooled_cohorts/cohort/all_cohorts.csv',\n",
    "        'base_path': '/labs/shahlab/projects/agataf/data/cohorts/pooled_cohorts',\n",
    "        'n_bootstrap': 5,\n",
    "        'eval_fold': 'test'\n",
    "       }\n",
    "\n",
    "cohort = pd.read_csv(args['cohort_path'])\n",
    "\n",
    "fair_evaluator = FairOVAEvaluator(thresholds = [0.075, 0.2],\n",
    "                                 metrics=['emd_ova', 'emd_1_ova', 'emd_0_ova'])\n",
    "\n",
    "standard_evaluator = StandardEvaluator(thresholds = [0.075, 0.2],\n",
    "                                              metrics = ['auc', 'auprc', 'loss_bce', \n",
    "                                                         'ace_rmse_logistic_log',\n",
    "                                                         ]\n",
    "                                             )\n",
    "eval_dict = {'label_var': 'labels',\n",
    "             'pred_prob_var': 'pred_probs',\n",
    "             'weight_var': 'weights',\n",
    "             'group_var_name': 'group',\n",
    "             'strata_vars': ['model_id', 'fold_id']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "remarkable-wound",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "for experiment in ['original_pce', 'revised_pce', 'apr14_erm', 'apr14_erm_recalib', 'apr14_mmd', 'apr14_thr']:\n",
    "    aggregate_path = os.path.join(args['base_path'], 'experiments', experiment, \n",
    "                                  'performance', 'all')\n",
    "    preds_path = os.path.join(aggregate_path, 'predictions.csv')\n",
    "    preds = pd.read_csv(preds_path).assign(experiment=experiment)\n",
    "    if 'model_id' not in preds.columns:\n",
    "        preds = preds.assign(model_id=0)\n",
    "    if 'fold_id' not in preds.columns:\n",
    "        preds = preds.assign(fold_id=0)\n",
    "    all_preds.append(preds)\n",
    "all_preds = pd.concat(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "relative-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_df = all_preds.query(\"(phase=='test') & (experiment==['original_pce', 'revised_pce', 'apr14_erm'])\")\n",
    "# evaluator = StandardEvaluator(metrics=['auc', 'loss_bce'])\n",
    "# result_df_ci = evaluator.bootstrap_evaluate(\n",
    "#     df=eval_df,\n",
    "#     n_boot=10,\n",
    "#     strata_vars_eval=['phase', 'model_type', 'model_id', 'fold_id', 'group'],\n",
    "#     strata_vars_boot=['phase', 'labels', 'group'],\n",
    "#     strata_var_replicate='fold_id',\n",
    "#     replicate_aggregation_mode=None,\n",
    "#     strata_var_experiment='model_id',\n",
    "#     baseline_experiment_name=0,\n",
    "#     strata_var_group='group',\n",
    "#     weight_var='weights',\n",
    "#     compute_overall=True,\n",
    "#    # group_overall_name='overall'\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-trailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dict = {'label_var': 'labels',\n",
    "             'pred_prob_var': 'pred_probs',\n",
    "             'weight_var': 'weights',\n",
    "             'group_var_name': 'group'}\n",
    "\n",
    "eval_overall_all = []\n",
    "eval_fair_ova_all = []\n",
    "for iter_idx in range(5): \n",
    "    cohort_bootstrap_sample = (cohort\n",
    "                               .query(\"fold_id=='test'\")\n",
    "                               .groupby(['ascvd_10yr', 'grp'])\n",
    "                               .sample(frac=1, replace=True)\n",
    "                               .person_id)\n",
    "    df_bootstrap = all_preds.loc[all_preds['person_id'].isin(cohort_bootstrap_sample)]\n",
    "    \n",
    "    eval_overall = standard_evaluator.get_result_df(df_bootstrap,\n",
    "                                                    strata_vars=['model_id', 'fold_id', 'experiment'],\n",
    "                                                    **eval_dict)\n",
    "    eval_fair_ova = fair_evaluator.get_result_df(df_bootstrap,\n",
    "                                                    strata_vars=['model_id', 'fold_id', 'experiment'],\n",
    "                                                    **eval_dict)\n",
    "    eval_overall_all.append(eval_overall.assign(bootstrap=iter_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-passport",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "fo\n",
    "\n",
    "\n",
    "aggregate_path = os.path.join(base_path, 'experiments', \n",
    "                              experiment_name, 'performance',\n",
    "                              'all')\n",
    "\n",
    "preds_path = os.path.join(aggregate_path, 'predictions.csv')\n",
    "\n",
    "    \n",
    "preds = pd.read_csv(preds_path)\n",
    "if 'fold_id' not in preds.columns:\n",
    "    preds = preds.assign(fold_id=0)\n",
    "if 'model_id' not in preds.columns:\n",
    "    preds = preds.assign(model_id=0)\n",
    "\n",
    "def get_calib_probs(model, x, transform=None):\n",
    "    \n",
    "    if transform=='log':\n",
    "        model_input = np.log(x)\n",
    "    else:\n",
    "        model_input = x\n",
    "        \n",
    "    calibration_density = model.predict_proba(model_input.reshape(-1, 1))[:, -1]\n",
    "                    \n",
    "    df = pd.DataFrame({'pred_probs': x,\n",
    "                       'model_input': model_input,\n",
    "                       'calibration_density': calibration_density})  \n",
    "    return df\n",
    "    \n",
    "def get_calib_model(labels, pred_probs, weights, transform=None):\n",
    "    \n",
    "    evaluator = CalibrationEvaluator()\n",
    "    _, model = evaluator.get_calibration_density_df(labels, \n",
    "                                                    pred_probs,\n",
    "                                                    weights,\n",
    "                                                    transform = transform)\n",
    "\n",
    "    return model\n",
    "\n",
    "df_to_calibrate = preds[preds.phase==eval_fold].reset_index(drop=True)\n",
    "lin_calibs=[]\n",
    "thr_calibs=[]\n",
    "model_type = preds.model_type.unique()[0]\n",
    "for iter_idx in range(n_bootstrap):\n",
    "    df_bootstrap = (df_to_calibrate\n",
    "                    .groupby(['group', 'labels', 'model_id', 'fold_id'])\n",
    "                    .sample(frac=1, replace=True))\n",
    "    for group in [1,2,3,4]:\n",
    "        for model_id in df_bootstrap.model_id.unique():\n",
    "            group_df = df_bootstrap.query(\"(group==@group) & (model_id==@model_id)\")\n",
    "            max_pred_prob = group_df.pred_probs.values.max()\n",
    "            \n",
    "            for fold_id in group_df.fold_id.unique(): \n",
    "                df = group_df.query(\"(fold_id==@fold_id)\")          \n",
    "\n",
    "                loop_kwargs = {'group': group,\n",
    "                               'fold_id': fold_id,\n",
    "                               'phase': eval_fold,\n",
    "                               'model_type': model_type,\n",
    "                               'model_id' : model_id}\n",
    "\n",
    "                model = get_calib_model(df.labels, df.pred_probs, df.weights, transform='log')\n",
    "                    \n",
    "                lin_calib = (get_calib_probs(model, np.append([1e-15], np.linspace(0.025, int(max_pred_prob/0.025)*0.025, int((max_pred_prob)/0.025))), 'log')\n",
    "                                 .assign(**loop_kwargs))\n",
    "                lin_calibs.append(lin_calib)\n",
    "                    \n",
    "                thr_calib = (get_calib_probs(model, [0.075, 0.2], 'log')\n",
    "                                 .assign(**loop_kwargs))\n",
    "                thr_calibs.append(thr_calib)\n",
    "    print(iter_idx)\n",
    "\n",
    "# lin_calibs = pd.concat(lin_calibs)\n",
    "# lin_calibs.to_csv(os.path.join(aggregate_path, 'calibration_sensitivity_test_raw.csv'), index=False)\n",
    "\n",
    "# thr_calibs = pd.concat(thr_calibs)\n",
    "# thr_calibs.to_csv(os.path.join(aggregate_path, 'calibration_sensitivity_thresholds_raw.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_to_calibrate\n",
    " .groupby(['group', 'labels', 'model_id', 'fold_id'])\n",
    " .sample(frac=1, replace=True)\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:utility]",
   "language": "python",
   "name": "conda-env-utility-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
