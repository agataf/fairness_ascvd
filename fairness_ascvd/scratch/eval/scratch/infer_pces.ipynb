{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "accessory-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import sys\n",
    "import copy\n",
    "import configargparse as argparse\n",
    "from prediction_utils.pytorch_utils.metrics import StandardEvaluator, FairOVAEvaluator\n",
    "from prediction_utils.util import yaml_write\n",
    "\n",
    "#import train_utils\n",
    "import yaml\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--experiment_name', type=str)\n",
    "# parser.add_argument('--cohort_path', type=str) \n",
    "# parser.add_argument('--result_path', type=str)\n",
    "# parser.add_argument('--base_config_path', type=str)\n",
    "base_path = '/labs/shahlab/projects/agataf/data/cohorts/pooled_cohorts'\n",
    "args = {'experiment_name': 'original_pce',\n",
    "       'cohort_path': os.path.join(base_path,'cohort/all_cohorts.csv'),\n",
    "        'base_path': base_path,\n",
    "       'base_config_path': os.path.join(base_path, 'experiments/basic_config.yaml'),\n",
    "       'result_path': os.path.join(base_path,'experiments/original_pce/performance')}\n",
    "\n",
    "\n",
    "# parser.set_defaults(\n",
    "#     save_outputs=False,\n",
    "#     run_evaluation=True,\n",
    "#     run_evaluation_group_standard=True,\n",
    "#     run_evaluation_group_fair_ova=True,\n",
    "#     print_debug=True,\n",
    "#     save_model_weights=False,\n",
    "#     data_query = '',\n",
    "#     num_epochs = 0\n",
    "# )\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# args = copy.deepcopy(args.__dict__)\n",
    "\n",
    "os.makedirs(args['result_path'], exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(args['cohort_path'])\n",
    "config_dict = yaml.load(open(args['base_config_path']), Loader=yaml.FullLoader)\n",
    "\n",
    "coefs = {1: [17.114, 0, 0.94, 0, -18.920, 4.475, 29.291, -6.432, 27.820, -6.087, 0.691, 0, 0.874],\n",
    "              2: [-29.799, 4.884, 13.54, -3.114, -13.578, 3.149, 2.019, 0, 1.957, 0, 7.574, -1.665, 0.661],\n",
    "              3: [2.469, 0, 0.302, 0, -0.307, 0, 1.916, 0, 1.809, 0, 0.549, 0, 0.645],\n",
    "              4: [12.344, 0, 11.853, -2.664, -7.990, 1.769, 1.797, 0, 1.7864, 0, 7.837, -1.795, 0.658]}\n",
    "mean_risk = {1: 86.61, 2:-29.18, 3:19.54, 4:61.18}\n",
    "baseline_survival = {1:0.9533, 2:0.9665, 3:0.8954, 4:0.9144}\n",
    "\n",
    "data_df = (pd.DataFrame({'log(age)': np.log(df.age),\n",
    "                    'log(age)^2': np.log(df.age)**2,\n",
    "                    'log(totchol)': np.log(df.totchol),\n",
    "                    'log(age)*log(totchol)': np.log(df.age)*np.log(df.totchol),\n",
    "                    'log(hdlc)': np.log(df.hdlc),\n",
    "                    'log(age)*log(hdlc)': np.log(df.age)*np.log(df.hdlc),\n",
    "                    'log(rxsbp)': np.log(df.rxsbp),\n",
    "                    'log(age)*log(rxsbp)': np.log(df.rxsbp)*np.log(df.age),\n",
    "                    'log(unrxsbp)': np.log(df.unrxsbp),\n",
    "                    'log(age)*log(unrxsbp)': np.log(df.unrxsbp)*np.log(df.age),\n",
    "                    'cursmoke': df.cursmoke,\n",
    "                    'log(age)*cursmoke': df.cursmoke*np.log(df.age),\n",
    "                    'diabt126': df.diabt126\n",
    "                   }\n",
    "                  ).replace(float('-inf'), 0)\n",
    "           # TODO: a warning is popping up here (RuntimeWarning: divide by zero encountered in log)\n",
    "     )\n",
    "\n",
    "risks = []\n",
    "for group in [1,2,3,4]:\n",
    "    relative_risk = (data_df\n",
    "                     .iloc[np.where(df.grp==group)]\n",
    "                     .multiply(coefs[group])\n",
    "                     .sum(axis=1)\n",
    "                     .sub(mean_risk[group])\n",
    "                     .transform(np.exp)\n",
    "                    )\n",
    "    risk = 1 - pow(baseline_survival[group], relative_risk)\n",
    "    risks.append(risk)\n",
    "    \n",
    "risks = pd.concat(risks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "generic-baptist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/labs/shahlab/projects/agataf/data/cohorts/pooled_cohorts/cohort/all_cohorts.csv'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args['cohort_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "unable-powder",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import KaplanMeierFitter, LogNormalFitter, WeibullFitter\n",
    "\n",
    "def censoring_weights(df, model_type = 'KM'):\n",
    "\n",
    "    if model_type == 'KM':\n",
    "        censoring_model = KaplanMeierFitter()\n",
    "    else:\n",
    "        raise ValueError(\"censoring_model not defined\")\n",
    "    \n",
    "    censoring_model.fit(df.query('is_train==1').event_time, 1.0*~df.query('is_train==1').event_indicator)\n",
    "    \n",
    "    weights = 1 / censoring_model.survival_function_at_times(df.event_time_10yr.values - 1e-5)\n",
    "    weights_dict = dict(zip(df.index.values, weights.values))\n",
    "    return weights_dict\n",
    "\n",
    "def get_censoring(df, by_group=True, model_type='KM'):\n",
    "    \n",
    "    if by_group:\n",
    "        weight_dict = {}\n",
    "        for group in [1, 2, 3, 4]:\n",
    "            group_df = df.query('grp==@group')\n",
    "            group_weights_dict = censoring_weights(group_df, model_type)\n",
    "            weight_dict.update(group_weights_dict)\n",
    "\n",
    "    else:\n",
    "        weight_dict = censoring_weights(cohort, censoring_model_type)\n",
    "\n",
    "    weights = pd.Series(weight_dict, name='weights') \n",
    "    return weights\n",
    "\n",
    "cohort = df.assign(is_train = lambda x: np.where((x.fold_id != config_dict.get('fold_id')) & (x.fold_id != \"test\") \n",
    "                                                         & (x.fold_id != \"eval\"),\n",
    "                                                         1, 0))\n",
    "all_weights = get_censoring(cohort, by_group = True, model_type = 'KM')\n",
    "cohort = cohort.join(all_weights)\n",
    "df = cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "younger-tractor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['9', '1', '2', 'test', '5', '4', '6', 'eval', '3', '7', '10', '8'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.fold_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "international-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df_eval = (df\n",
    "           .rename(columns={'person_id': 'row_id',\n",
    "                            'fold_id': 'phase',\n",
    "                            'grp': 'group'})\n",
    "           .assign(labels = lambda x: x.ascvd_10yr.astype(int),\n",
    "                   pred_probs = risks)\n",
    "           .filter(['phase', 'pred_probs', 'labels', 'row_id', 'weights', 'group'])\n",
    "            )\n",
    "\n",
    "output_df_eval.to_parquet(\n",
    "    os.path.join(args['result_path'], \"output_df.parquet\"),\n",
    "    index=False,\n",
    "    engine=\"pyarrow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "active-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_path =  os.path.join(args['base_path'], 'experiments', \n",
    "                              args['experiment_name'], 'performance',\n",
    "                              'all')\n",
    "\n",
    "os.makedirs(aggregate_path, exist_ok=True)\n",
    "\n",
    "output_df_eval.to_csv(\n",
    "    os.path.join(aggregate_path, 'predictions.csv'),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "olympic-miniature",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = StandardEvaluator(threshold_metrics = config_dict['logging_threshold_metrics'],\n",
    "                              thresholds = config_dict['logging_thresholds'],\n",
    "                              metrics = ['auc', 'auprc', 'loss_bce', \n",
    "                                         'ace_rmse_logistic_log',\n",
    "                                         'ace_abs_logistic_log']\n",
    "                             )\n",
    "\n",
    "eval_general_args = {'df': output_df_eval,\n",
    "                     'label_var': 'labels',\n",
    "                     'pred_prob_var': 'pred_probs',\n",
    "                     'weight_var': 'weights', \n",
    "                     'strata_vars': ['phase'],\n",
    "                     'group_var_name': 'group'}\n",
    "\n",
    "result_df_overall = evaluator.get_result_df(**eval_general_args)\n",
    "\n",
    "\n",
    "results_df = (result_df_overall\n",
    "              .assign(group='all')\n",
    "              .rename(columns={'performance_overall': 'performance'})\n",
    "              #.append(result_df_by_group)\n",
    ")\n",
    "\n",
    "result_df_overall.to_parquet(\n",
    "    os.path.join(args['result_path'], \"result_df_group_standard_eval.parquet\"),\n",
    "    engine=\"pyarrow\",\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fancy-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = FairOVAEvaluator(threshold_metrics = config_dict['logging_threshold_metrics'],\n",
    "                             thresholds = config_dict['logging_thresholds'])\n",
    "    \n",
    "eval_fair_args = {'df': output_df_eval,\n",
    "                  'label_var': 'labels',\n",
    "                  'pred_prob_var': 'pred_probs',\n",
    "                  'weight_var': 'weights',\n",
    "                  'group_var_name': 'group',\n",
    "                  'strata_vars': ['phase']}\n",
    "        \n",
    "result_df_group_fair_ova = evaluator.get_result_df(**eval_fair_args)\n",
    "\n",
    "result_df_group_fair_ova.to_parquet(\n",
    "    os.path.join(args['result_path'], \"result_df_group_fair_ova.parquet\"),\n",
    "    engine=\"pyarrow\",\n",
    "    index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:utility]",
   "language": "python",
   "name": "conda-env-utility-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
