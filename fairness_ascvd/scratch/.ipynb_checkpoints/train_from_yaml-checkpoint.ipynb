{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "import configargparse as argparse\n",
    "\n",
    "from prediction_utils.util import yaml_write\n",
    "from prediction_utils.pytorch_utils.models import TorchModel\n",
    "from prediction_utils.pytorch_utils.lagrangian import MultiLagrangianThresholdRateModel\n",
    "from prediction_utils.pytorch_utils.robustness import GroupDROModel\n",
    "\n",
    "from prediction_utils.pytorch_utils.metrics import StandardEvaluator, FairOVAEvaluator, CalibrationEvaluator\n",
    "\n",
    "import git\n",
    "repo = git.Repo('.', search_parent_directories=True)\n",
    "os.chdir(repo.working_dir) \n",
    "\n",
    "import train_utils\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'erm'\n",
    "config_id = '00'\n",
    "fold_id = '0'\n",
    "BASE_PATH = '/labs/shahlab/projects/agataf/data/cohorts/pooled_cohorts'\n",
    "args = {'experiment_name': 'erm',\n",
    "        'cohort_path': '/labs/shahlab/projects/agataf/data/pooled_cohorts/cohort_extraction/all_cohorts.csv',\n",
    "        'base_path': '/labs/shahlab/projects/agataf/data/cohorts/pooled_cohorts',\n",
    "        'config_id': '00',\n",
    "        'fold_id': '0',\n",
    "        'print_debug': True,\n",
    "        'save_outputs': True,\n",
    "        'run_evaluation_group_standard': True,\n",
    "        'run_evaluation_group_fair_ova': True,\n",
    "        'save_model_weights': True,\n",
    "        'run_evaluation': True,\n",
    "        #'split_gender': True,\n",
    "        #'data_query': 'gender_male==0'\n",
    "       }\n",
    "\n",
    "\n",
    "BASE_CONFIG_PATH = os.path.join(args['base_path'], 'experiments', args['experiment_name'], 'basic_config.yaml')\n",
    "CONFIG_PATH = os.path.join(args['base_path'], 'experiments', args['experiment_name'], 'config',\n",
    "                           '.'.join((args['config_id'], 'yaml')))\n",
    "\n",
    "RESULT_PATH = os.path.join(args['base_path'], 'experiments', args['experiment_name'], 'performance',\n",
    "                           '.'.join((args['config_id'], 'yaml')), args['fold_id'])\n",
    "LOGGING_PATH = os.path.join(RESULT_PATH, 'training_log.log')\n",
    "\n",
    "args.update({'result_path': RESULT_PATH})\n",
    "# following https://github.com/som-shahlab/group_robustness_fairness/blob/main/group_robustness_fairness/scripts/tune_baseline_model_starr.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INITIAL SETUP #####\n",
    "\n",
    "os.makedirs(RESULT_PATH, exist_ok=True)\n",
    "\n",
    "model_params = yaml.load(open(CONFIG_PATH), Loader=yaml.FullLoader)\n",
    "config_dict = yaml.load(open(BASE_CONFIG_PATH), Loader=yaml.FullLoader)\n",
    "\n",
    "config_dict.update(model_params)\n",
    "config_dict.update({'logging_path': LOGGING_PATH})\n",
    "\n",
    "## remove ##\n",
    "config_dict['num_epochs'] = 5\n",
    "\n",
    "logger = train_utils.logger_setup(config_dict, args)\n",
    "\n",
    "##### DATASET #####\n",
    "data_df = pd.read_csv(args['cohort_path'])\n",
    "\n",
    "if (len(args['data_query']) > 0):\n",
    "    data_df = (data_df\n",
    "               .query(args['data_query'])\n",
    "               .reset_index(drop=True)\n",
    "              )\n",
    "    \n",
    "data_args = train_utils.get_dict_subset(config_dict, ['feature_columns', 'val_fold_id', 'test_fold_id', 'batch_size'])\n",
    "data = train_utils.Dataset(data_df, deg=2, **data_args)\n",
    "\n",
    "# add input dim to dictionary\n",
    "config_dict.update({'input_dim': data.features_dict_uncensored_scaled['train'].shape[1]})\n",
    "\n",
    "# log\n",
    "logger.info(\"Result path: {}\".format(args['result_path']))\n",
    "\n",
    "model, logger = train_utils.model_setup(config_dict, logger, args)\n",
    "\n",
    "result_df = model.train(loaders=data.loaders_dict)['performance']\n",
    "\n",
    "result_df.to_parquet(os.path.join(RESULT_PATH, \"result_df_training.parquet\"), index=False, engine=\"pyarrow\")\n",
    "\n",
    "if args['save_model_weights']:\n",
    "    torch.save(model.model.state_dict(), os.path.join(RESULT_PATH, \"state_dict.pt\"))\n",
    "    \n",
    "if args['run_evaluation']:\n",
    "    logger.info(\"Evaluating model\")\n",
    "\n",
    "    predict_dict = model.predict(data.loaders_dict_predict, \n",
    "                                 phases=['val', 'test'])\n",
    "    \n",
    "    logger = train_utils.evaluation(predict_dict, args, config_dict, logger)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
