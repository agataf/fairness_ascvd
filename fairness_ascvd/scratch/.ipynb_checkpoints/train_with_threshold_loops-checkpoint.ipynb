{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "import copy\n",
    "import configargparse as argparse\n",
    "\n",
    "from prediction_utils.util import yaml_write\n",
    "from prediction_utils.pytorch_utils.models import TorchModel\n",
    "from prediction_utils.pytorch_utils.lagrangian import MultiLagrangianThresholdRateModel\n",
    "from prediction_utils.pytorch_utils.robustness import GroupDROModel\n",
    "\n",
    "from prediction_utils.pytorch_utils.metrics import StandardEvaluator, FairOVAEvaluator, CalibrationEvaluator\n",
    "\n",
    "import git\n",
    "repo = git.Repo('.', search_parent_directories=True)\n",
    "os.chdir(repo.working_dir) \n",
    "\n",
    "import train_utils\n",
    "import yaml\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--experiment_name', type=str)\n",
    "# parser.add_argument('--cohort_path', type=str) \n",
    "# parser.add_argument('--result_path', type=str)\n",
    "# parser.add_argument('--logging_path', type=str)\n",
    "# # parser.add_argument('--base_path', type=str)\n",
    "# parser.add_argument('--config_id', type=str)\n",
    "# parser.add_argument('--fold_id', type=str)\n",
    "# parser.add_argument('--print_debug', type=bool)\n",
    "# parser.add_argument('--save_outputs', type=bool)\n",
    "# parser.add_argument('--run_evaluation', type=bool)\n",
    "# parser.add_argument('--run_evaluation_group_standard', type=bool)\n",
    "# parser.add_argument('--run_evaluation_group_fair_ova', type=bool)\n",
    "# parser.add_argument('--save_model_weights', type=bool)\n",
    "# parser.add_argument('--data_query', type=str)\n",
    "# parser.add_argument('--base_config_path', type=str)\n",
    "# parser.add_argument('--config_path', type=str)\n",
    "# parser.add_argument('--num_epochs', type=int)\n",
    "\n",
    "# parser.set_defaults(\n",
    "#     save_outputs=False,\n",
    "#     run_evaluation=True,\n",
    "#     run_evaluation_group_standard=True,\n",
    "#     run_evaluation_group_fair_ova=True,\n",
    "#     print_debug=True,\n",
    "#     save_model_weights=False,\n",
    "#     data_query = '',\n",
    "#     num_epochs = 0\n",
    "# )\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# args = copy.deepcopy(args.__dict__)\n",
    "\n",
    "\n",
    "def run_model(args, config_dict):\n",
    "    \n",
    "    ##### INITIAL SETUP #####\n",
    "    os.makedirs(args['result_path'], exist_ok=True)\n",
    "\n",
    "    logger = train_utils.logger_setup(config_dict, args)\n",
    "\n",
    "    ##### DATASET #####\n",
    "    data_df = pd.read_csv(args['cohort_path'])\n",
    "\n",
    "    if (len(args['data_query']) > 0):\n",
    "        data_df = (data_df\n",
    "                   .query(args['data_query'])\n",
    "                   .reset_index(drop=True)\n",
    "                  )\n",
    "\n",
    "    data_args = train_utils.get_dict_subset(config_dict, ['feature_columns', 'val_fold_id', 'test_fold_id', 'batch_size'])\n",
    "    data = train_utils.Dataset(data_df, deg=2, **data_args)\n",
    "\n",
    "    # add input dim to dictionary\n",
    "    config_dict.update({'input_dim': data.features_dict_uncensored_scaled['train'].shape[1]})\n",
    "\n",
    "    # log\n",
    "    logger.info(\"Result path: {}\".format(args['result_path']))\n",
    "\n",
    "    model, logger = train_utils.model_setup(config_dict, logger, args)\n",
    "\n",
    "    result_df = model.train(loaders=data.loaders_dict)['performance']\n",
    "\n",
    "    result_df.to_parquet(os.path.join(args['result_path'], \"result_df_training.parquet\"), index=False, engine=\"pyarrow\")\n",
    "\n",
    "    if args['save_model_weights']:\n",
    "        torch.save(model.model.state_dict(), os.path.join(args['result_path'], \"state_dict.pt\"))\n",
    "\n",
    "    if args['run_evaluation']:\n",
    "        logger.info(\"Evaluating model\")\n",
    "\n",
    "        predict_dict = model.predict(data.loaders_dict_predict, \n",
    "                                     phases=['val', 'test'])\n",
    "\n",
    "        # general evaluation\n",
    "        output_df_eval, result_df_eval = (\n",
    "            predict_dict[\"outputs\"],\n",
    "            predict_dict[\"performance\"]\n",
    "        )\n",
    "\n",
    "        logger.info(result_df_eval)\n",
    "\n",
    "        output_df_eval = (train_utils.add_ranges(output_df_eval)\n",
    "                          .rename(columns={'row_id': 'person_id'})\n",
    "                          .merge(data_df.filter(['person_id', 'ldlc']), how='inner', on='person_id')\n",
    "                          .assign(relative_risk = lambda x: train_utils.treat_relative_risk(x),\n",
    "                                  new_risk = lambda x: x.pred_probs*x.relative_risk\n",
    "                                 )\n",
    "                      )\n",
    "\n",
    "        # Dump evaluation result to disk\n",
    "        result_df_eval.to_parquet(\n",
    "            os.path.join(args['result_path'], \"result_df_training_eval.parquet\"),\n",
    "            index=False,\n",
    "            engine=\"pyarrow\",\n",
    "        )\n",
    "\n",
    "        if args.get('save_outputs'):\n",
    "            output_df_eval.to_parquet(\n",
    "                os.path.join(args['result_path'], \"output_df.parquet\"),\n",
    "                index=False,\n",
    "                engine=\"pyarrow\",\n",
    "            )\n",
    "\n",
    "        logger = train_utils.evaluation(output_df_eval, args, config_dict, logger)\n",
    "        \n",
    "EXPERIMENT_NAME = 'eq_oddsconstr'\n",
    "\n",
    "BASE_PATH = '/labs/shahlab/projects/agataf/data/cohorts/pooled_cohorts'\n",
    "args = {'experiment_name': EXPERIMENT_NAME,\n",
    "        'cohort_path': '/labs/shahlab/projects/agataf/data/pooled_cohorts/cohort_extraction/all_cohorts.csv',\n",
    "        'base_path': BASE_PATH,\n",
    "        'print_debug': True,\n",
    "        'save_outputs': True,\n",
    "        'run_evaluation_group_standard': True,\n",
    "        'run_evaluation_group_fair_ova': True,\n",
    "        'save_model_weights': True,\n",
    "        'run_evaluation': True,\n",
    "        'split_gender': False,\n",
    "        'data_query': ''\n",
    "       }\n",
    "\n",
    "\n",
    "BASE_CONFIG_PATH = os.path.join(BASE_PATH, 'experiments', 'basic_config.yaml')\n",
    "config_dict = yaml.load(open(BASE_CONFIG_PATH), Loader=yaml.FullLoader)\n",
    "\n",
    "update_dict = {\n",
    "    \"threshold_mode\": \"conditional\",\n",
    "    \"thresholds\": [0.075, 0.2],\n",
    "    \"surrogate_scale\": 1.0,\n",
    "    'logging_metrics': ['auc', 'auprc', 'brier', 'loss_bce'],\n",
    "    'data_query': '',\n",
    "    'group_objective_type': 'multiThreshold',\n",
    "    'evaluate_by_group': True,\n",
    "    'sparse': False,\n",
    "    'output_dim': 2,\n",
    "    \"num_groups\": 4,\n",
    "    'num_hidden': 0,\n",
    "    'weighted_loss': True,\n",
    "    'num_epochs': 10\n",
    "}\n",
    "\n",
    "config_dict.update(update_dict)\n",
    "\n",
    "configs = zip(['00', '01', '02', '03', '04', '05', '06', '07', '08', '09'], np.geomspace(1e-3,1e-1,num=10))\n",
    "\n",
    "for config_id, lambda_reg in configs:\n",
    "\n",
    "    for fold_id in range(1,11):\n",
    "\n",
    "        RESULT_PATH = os.path.join(args['base_path'], 'experiments', args['experiment_name'], 'performance',\n",
    "                                   '.'.join((str(config_id), 'yaml')), str(fold_id))\n",
    "        LOGGING_PATH = os.path.join(RESULT_PATH, 'training_log.log')\n",
    "\n",
    "        config_dict.update({'val_fold_id': str(fold_id), \n",
    "                            'num_epochs': 100, \n",
    "                            'lambda_group_regularization': lambda_reg,\n",
    "                            'logging_path': LOGGING_PATH})\n",
    "\n",
    "\n",
    "        args.update({'result_path': RESULT_PATH})\n",
    "\n",
    "        run_model(args, config_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
