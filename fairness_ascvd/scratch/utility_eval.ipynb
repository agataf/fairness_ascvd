{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "import configargparse as argparse\n",
    "import math\n",
    "\n",
    "from prediction_utils.util import yaml_write\n",
    "\n",
    "\n",
    "from prediction_utils.pytorch_utils.metrics import StandardEvaluator, FairOVAEvaluator, CalibrationEvaluator\n",
    "\n",
    "# import git\n",
    "# repo = git.Repo('.', search_parent_directories=True)\n",
    "# os.chdir(repo.working_dir) \n",
    "\n",
    "import train_utils\n",
    "import yaml\n",
    "\n",
    "args = {'experiment_name': 'original_pce',\n",
    "        'cohort_path': '/labs/shahlab/projects/agataf/data/pooled_cohorts/cohort_extraction/all_cohorts.csv',\n",
    "        'base_path': '/labs/shahlab/projects/agataf/data/cohorts/pooled_cohorts',\n",
    "        'save': False\n",
    "       }\n",
    "\n",
    "\n",
    "BASE_CONFIG_PATH = os.path.join(args['base_path'], 'experiments', 'basic_config.yaml')\n",
    "\n",
    "RESULT_PATH = os.path.join(args['base_path'], 'experiments', args['experiment_name'], 'performance')\n",
    "if args['save']:\n",
    "    os.makedirs(RESULT_PATH, exist_ok=True)\n",
    "\n",
    "args.update({'result_path': RESULT_PATH})\n",
    "config_dict = yaml.load(open(BASE_CONFIG_PATH), Loader=yaml.FullLoader)\n",
    "\n",
    "\n",
    "df = pd.read_csv(args['cohort_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ranges(df, one_hot=False, threshold1 = 0.075, threshold2 = 0.2):\n",
    "    \n",
    "    range1 = (df.pred_probs < threshold1).astype(int)\n",
    "    range2 = ((df.pred_probs >= threshold1) & (df.pred_probs < threshold2)).astype(int)\n",
    "    range3 = ((df.pred_probs >= threshold2)).astype(int)\n",
    "\n",
    "    if one_hot:\n",
    "        df = df.assign(treat0=range1, treat1=range2, treat2=range3)\n",
    "    else:\n",
    "        rang = 1*range2 + 2*range3\n",
    "        df = df.assign(treat=rang)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def treat_relative_risk(df):\n",
    "    ldlc_reductions_by_treatment = {0: 1, 1: 0.7, 2: 0.5}\n",
    "    relative_risk_statin = 0.75\n",
    "\n",
    "    absolute_ldlc_reduction = df.ldlc*(1-df.treat.map(ldlc_reductions_by_treatment))\n",
    "\n",
    "    return [math.pow(relative_risk_statin, el/38.7) for el in absolute_ldlc_reduction]\n",
    "\n",
    "predictions = pd.read_parquet(os.path.join(args['result_path'], \"output_df.parquet\"),engine=\"pyarrow\")\n",
    "\n",
    "predictions = (add_ranges(predictions)\n",
    "               .rename(columns={'row_id': 'person_id'})\n",
    "               .merge(df.filter(['person_id', 'ldlc']), how='outer', on='person_id')\n",
    "               .assign(relative_risk = lambda x: treat_relative_risk(x),\n",
    "                      new_risk = lambda x: x.pred_probs*x.relative_risk)\n",
    "              )\n",
    "absolute_rr = predictions.pred_probs-new_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_specificity_at_threshold(threshold, weighted=False):\n",
    "    \"\"\"\n",
    "    Returns a lambda function that computes the specificity at a provided threshold.\n",
    "    If weights = True, the lambda function takes a third argument for the sample weights\n",
    "    \"\"\"\n",
    "    if not weighted:\n",
    "        return (\n",
    "            lambda labels, pred_probs: (\n",
    "                (labels == 0) & (labels == (pred_probs >= threshold))\n",
    "            ).sum()\n",
    "            / (labels == 0).sum()\n",
    "            if (labels == 0).sum() > 0\n",
    "            else 0.0\n",
    "        )\n",
    "    else:\n",
    "        return (\n",
    "            lambda labels, pred_probs, sample_weight: (\n",
    "                ((labels == 0) & (labels == (pred_probs >= threshold))) * sample_weight\n",
    "            ).sum()\n",
    "            / ((labels == 0) * sample_weight).sum()\n",
    "            if (labels == 0).sum() > 0\n",
    "            else 0.0\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
