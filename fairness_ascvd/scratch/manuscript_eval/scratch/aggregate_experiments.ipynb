{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "import sys\n",
    "#import torch\n",
    "import copy\n",
    "import yaml\n",
    "\n",
    "### argparse ###\n",
    "#EXPERIMENT_NAME = 'apr14_mmd'\n",
    "#EXPERIMENT_NAME = 'apr14_thr'\n",
    "EXPERIMENT_NAME = 'apr14_erm_recalib'\n",
    "\n",
    "args = {'experiment_name': EXPERIMENT_NAME,\n",
    "        'cohort_path': '/labs/shahlab/projects/agataf/data/cohorts/pooled_cohorts/cohort/all_cohorts.csv',\n",
    "        'base_path': '/labs/shahlab/projects/agataf/data/cohorts/pooled_cohorts'\n",
    "       }\n",
    "###############\n",
    "\n",
    "\n",
    "# experiment_configs = pd.read_csv(os.path.join(args['base_path'], 'experiments', \n",
    "#                                               args['experiment_name'], 'config', 'config.csv')\n",
    "#                                 )\n",
    "# config_ids = experiment_configs.id.values\n",
    "\n",
    "aggregate_path = os.path.join(args['base_path'], 'experiments', args['experiment_name'], 'performance', 'all')\n",
    "os.makedirs(aggregate_path, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "/labs/shahlab/projects/agataf/data/cohorts/pooled_cohorts/experiments/apr14_erm_recalib/performance/all/result_df_group_standard_eval.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c3a95dd4211e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#             lambda_reg = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m result_df_group_standard_eval = (pd.read_parquet(\n\u001b[0m\u001b[1;32m     19\u001b[0m             os.path.join(RESULT_PATH,\n\u001b[1;32m     20\u001b[0m                          \u001b[0;34m'result_df_group_standard_eval.parquet'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/labs/shahlab/envs/agataf/miniconda3/envs/utility/lib/python3.8/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \"\"\"\n\u001b[1;32m    316\u001b[0m     \u001b[0mimpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/labs/shahlab/envs/agataf/miniconda3/envs/utility/lib/python3.8/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_pandas_metadata\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         result = self.api.parquet.read_table(\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilesystem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         ).to_pandas()\n",
      "\u001b[0;32m/labs/shahlab/envs/agataf/miniconda3/envs/utility/lib/python3.8/site-packages/pyarrow/parquet.py\u001b[0m in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, metadata, use_pandas_metadata, memory_map, read_dictionary, filesystem, filters, buffer_size, partitioning, use_legacy_dataset, ignore_prefixes)\u001b[0m\n\u001b[1;32m   1670\u001b[0m             )\n\u001b[1;32m   1671\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1672\u001b[0;31m             dataset = _ParquetDatasetV2(\n\u001b[0m\u001b[1;32m   1673\u001b[0m                 \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m                 \u001b[0mfilesystem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilesystem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/labs/shahlab/envs/agataf/miniconda3/envs/utility/lib/python3.8/site-packages/pyarrow/parquet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_paths, filesystem, filters, partitioning, read_dictionary, buffer_size, memory_map, ignore_prefixes, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m                 infer_dictionary=True)\n\u001b[1;32m   1531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m         self._dataset = ds.dataset(path_or_paths, filesystem=filesystem,\n\u001b[0m\u001b[1;32m   1533\u001b[0m                                    \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparquet_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m                                    \u001b[0mpartitioning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/labs/shahlab/envs/agataf/miniconda3/envs/utility/lib/python3.8/site-packages/pyarrow/dataset.py\u001b[0m in \u001b[0;36mdataset\u001b[0;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0;31m# TODO(kszucs): support InMemoryDataset for a table input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_filesystem_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_is_path_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/labs/shahlab/envs/agataf/miniconda3/envs/utility/lib/python3.8/site-packages/pyarrow/dataset.py\u001b[0m in \u001b[0;36m_filesystem_dataset\u001b[0;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths_or_selector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_multiple_sources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilesystem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths_or_selector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_single_source\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilesystem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     options = FileSystemFactoryOptions(\n",
      "\u001b[0;32m/labs/shahlab/envs/agataf/miniconda3/envs/utility/lib/python3.8/site-packages/pyarrow/dataset.py\u001b[0m in \u001b[0;36m_ensure_single_source\u001b[0;34m(path, filesystem)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mpaths_or_selector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfilesystem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths_or_selector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: /labs/shahlab/projects/agataf/data/cohorts/pooled_cohorts/experiments/apr14_erm_recalib/performance/all/result_df_group_standard_eval.parquet"
     ]
    }
   ],
   "source": [
    "standard_eval = []\n",
    "outputs=[]\n",
    "# for config_id in config_ids:\n",
    "#     for fold_id in range(1,11):\n",
    "RESULT_PATH = os.path.join(args['base_path'], 'experiments', args['experiment_name'], 'performance',\n",
    "                                           'all')\n",
    "# LOGGING_PATH = os.path.join(RESULT_PATH, 'training_log.log')\n",
    "        \n",
    "# CONFIG_PATH = os.path.join(args['base_path'], 'experiments', args['experiment_name'], 'config',\n",
    "#                                                    '.'.join((str(config_id), 'yaml')))\n",
    "\n",
    "\n",
    "#         config = yaml.load(open(CONFIG_PATH), Loader=yaml.SafeLoader)\n",
    "#         lambda_reg = config.get('lambda_group_regularization')\n",
    "#         if lambda_reg is None:\n",
    "#             lambda_reg = 0\n",
    "\n",
    "result_df_group_standard_eval = (pd.read_parquet(\n",
    "            os.path.join(RESULT_PATH,\n",
    "                         'result_df_group_standard_eval.parquet'\n",
    "                        )))\n",
    "#                                          .assign(fold_id=fold_id,\n",
    "#                                                  #config_id=config_id,\n",
    "#                                                  lambda_reg=round(lambda_reg, 3)))\n",
    "standard_eval.append(result_df_group_standard_eval)\n",
    "        \n",
    "output_df = (\n",
    "            pd.read_parquet(\n",
    "                os.path.join(RESULT_PATH,\n",
    "                             'output_df.parquet'\n",
    "                            )\n",
    "            ))\n",
    "#             .assign(fold_id=fold_id,\n",
    "#                     config_id=config_id,\n",
    "#                     lambda_reg=round(lambda_reg, 3))\n",
    "#         )\n",
    "        \n",
    "outputs.append(output_df)\n",
    "\n",
    "df = pd.concat(standard_eval)\n",
    "preds = pd.concat(outputs)\n",
    "\n",
    "df.to_csv(os.path.join(aggregate_path, 'standard_evaluation.csv'), index=False)\n",
    "preds.to_csv(os.path.join(aggregate_path, 'predictions.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_eval = []\n",
    "outputs=[]\n",
    "for config_id in config_ids:\n",
    "    for fold_id in range(1,11):\n",
    "        RESULT_PATH = os.path.join(args['base_path'], 'experiments', args['experiment_name'], 'performance',\n",
    "                                           '.'.join((str(config_id), 'yaml')), str(fold_id))\n",
    "        LOGGING_PATH = os.path.join(RESULT_PATH, 'training_log.log')\n",
    "        \n",
    "        CONFIG_PATH = os.path.join(args['base_path'], 'experiments', args['experiment_name'], 'config',\n",
    "                                                   '.'.join((str(config_id), 'yaml')))\n",
    "\n",
    "\n",
    "        config = yaml.load(open(CONFIG_PATH), Loader=yaml.SafeLoader)\n",
    "        lambda_reg = config.get('lambda_group_regularization')\n",
    "        if lambda_reg is None:\n",
    "            lambda_reg = 0\n",
    "\n",
    "        result_df_group_standard_eval = (pd.read_parquet(\n",
    "            os.path.join(RESULT_PATH,\n",
    "                         'result_df_group_standard_eval.parquet'\n",
    "                        ))\n",
    "                                         .assign(fold_id=fold_id,\n",
    "                                                 config_id=config_id,\n",
    "                                                 lambda_reg=round(lambda_reg, 3)))\n",
    "        standard_eval.append(result_df_group_standard_eval)\n",
    "        \n",
    "        output_df = (\n",
    "            pd.read_parquet(\n",
    "                os.path.join(RESULT_PATH,\n",
    "                             'output_df.parquet'\n",
    "                            )\n",
    "            )\n",
    "            .assign(fold_id=fold_id,\n",
    "                    config_id=config_id,\n",
    "                    lambda_reg=round(lambda_reg, 3))\n",
    "        )\n",
    "        \n",
    "        outputs.append(output_df)\n",
    "\n",
    "df = pd.concat(standard_eval)\n",
    "preds = pd.concat(outputs)\n",
    "\n",
    "df.to_csv(os.path.join(aggregate_path, 'standard_evaluation.csv'), index=False)\n",
    "preds.to_csv(os.path.join(aggregate_path, 'predictions.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:utility]",
   "language": "python",
   "name": "conda-env-utility-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
