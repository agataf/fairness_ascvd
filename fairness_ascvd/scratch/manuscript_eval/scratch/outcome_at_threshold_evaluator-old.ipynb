{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "valued-gauge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "grp_label_dict = {1: \"Black women\", 2: \"White women\", 3: \"Black men\", 4: \"White men\"}\n",
    "\n",
    "args = {\n",
    "    \"experiment_name\": \"apr14_erm\",\n",
    "    \"cohort_path\": \"/labs/shahlab/projects/agataf/data/cohorts/pooled_cohorts/cohort/all_cohorts.csv\",\n",
    "    \"base_path\": \"/labs/shahlab/projects/agataf/data/cohorts/pooled_cohorts\",\n",
    "    \"eval_fold\": \"test\",\n",
    "}\n",
    "aggregate_path = os.path.join(\n",
    "    args[\"base_path\"], \"experiments\", args[\"experiment_name\"], \"performance\", \"all\"\n",
    ")\n",
    "\n",
    "preds_path = os.path.join(aggregate_path, \"predictions.csv\")\n",
    "preds = pd.read_csv(preds_path)\n",
    "eval_df = preds.query('phase == \"test\"')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "technological-ceramic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import itertools\n",
    "import scipy\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    brier_score_loss,\n",
    "    log_loss,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from prediction_utils.util import df_dict_concat\n",
    "from collections import ChainMap\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\"\"\"\n",
    "Evaluators\n",
    "    - StandardEvaluator\n",
    "        - Computes standard model performance metrics\n",
    "    - FairOVAEvaluator\n",
    "        - Computes fairness metrics in one-vs-all (OVA) fashion\n",
    "    - CalibrationEvaluator\n",
    "        - Computes calibration metrics (absolute and relative calibration error) \n",
    "            using logistic regression as an auxiliary estimator\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class StandardEvaluator:\n",
    "    def __init__(self, metrics=None, threshold_metrics=None, thresholds=None):\n",
    "        # default behavior: use all metrics, do not use any threshold metrics\n",
    "        if metrics is None:\n",
    "            self.metrics = self.get_default_threshold_free_metrics()\n",
    "        else:\n",
    "            self.metrics = metrics\n",
    "        self.thresholds = thresholds\n",
    "\n",
    "        if self.thresholds is not None:\n",
    "            assert isinstance(self.thresholds, list)\n",
    "            self.thresholds = [float(x) for x in self.thresholds]\n",
    "\n",
    "        self.threshold_metrics = threshold_metrics\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        df,\n",
    "        strata_vars=None,\n",
    "        result_name=\"performance\",\n",
    "        weight_var=None,\n",
    "        label_var=\"labels\",\n",
    "        pred_prob_var=\"pred_probs\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Evaluates predictions against a set of labels with a set of metric functions\n",
    "        Arguments:\n",
    "            df: a dataframe with one row per prediction\n",
    "            result_name: a string that will be used to label the metric values in the result\n",
    "            weight_var: a string identifier for sample weights in df\n",
    "            label_var: a string identifier for the outcome labels in df\n",
    "            pred_prob_var: a string identifier for the predicted probabilities in df\n",
    "        \"\"\"\n",
    "        metric_fns = self.get_metric_fns(\n",
    "            metrics=self.metrics,\n",
    "            threshold_metrics=self.threshold_metrics,\n",
    "            thresholds=self.thresholds,\n",
    "            weighted=weight_var is not None,\n",
    "        )\n",
    "        print(metric_fns)\n",
    "        if df[pred_prob_var].dtype == \"float32\":\n",
    "            df[pred_prob_var] = df[pred_prob_var].astype(np.float64)\n",
    "\n",
    "        if strata_vars is not None:\n",
    "            strata_vars = [var for var in strata_vars if var in df.columns]\n",
    "        if (strata_vars is None) or (len(strata_vars) == 0):\n",
    "            result_df = (\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        metric: metric_fn(\n",
    "                            df[label_var].values, df[pred_prob_var].values\n",
    "                        )\n",
    "                        if weight_var is None\n",
    "                        else metric_fn(\n",
    "                            df[label_var].values,\n",
    "                            df[pred_prob_var].values,\n",
    "                            sample_weight=df[weight_var].values,\n",
    "                        )\n",
    "                        for metric, metric_fn in metric_fns.items()\n",
    "                    },\n",
    "                    index=[result_name],\n",
    "                )\n",
    "                .transpose()\n",
    "                .rename_axis(\"metric\")\n",
    "                .reset_index()\n",
    "            )\n",
    "        else:\n",
    "            result_df = df_dict_concat(\n",
    "                {\n",
    "                    metric: df.groupby(strata_vars)\n",
    "                    .apply(\n",
    "                        lambda x: metric_func(\n",
    "                            x[label_var].values, x[pred_prob_var].values\n",
    "                        )\n",
    "                        if weight_var is None\n",
    "                        else metric_func(\n",
    "                            x[label_var].values,\n",
    "                            x[pred_prob_var].values,\n",
    "                            sample_weight=x[weight_var].values,\n",
    "                        )\n",
    "                    )\n",
    "                    .rename(index=result_name)\n",
    "                    .rename_axis(strata_vars)\n",
    "                    .reset_index()\n",
    "                    for metric, metric_func in metric_fns.items()\n",
    "                },\n",
    "                \"metric\",\n",
    "            )\n",
    "        return result_df\n",
    "\n",
    "    def get_result_df(\n",
    "        self,\n",
    "        df,\n",
    "        strata_vars=None,\n",
    "        weight_var=None,\n",
    "        label_var=\"labels\",\n",
    "        pred_prob_var=\"pred_probs\",\n",
    "        group_var_name=\"group\",\n",
    "        result_name=\"performance\",\n",
    "        compute_group_min_max=False,\n",
    "        group_overall_name=\"overall\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A convenience function that calls evaluate with and without stratifying on group_var_name\n",
    "        \"\"\"\n",
    "        if strata_vars is not None:\n",
    "            strata_vars = [var for var in strata_vars if var in df.columns]\n",
    "        else:\n",
    "            strata_vars = []\n",
    "\n",
    "        if group_overall_name in (df[group_var_name].unique()):\n",
    "            raise ValueError(\"group_overall_name must not be a defined group\")\n",
    "\n",
    "        if group_var_name in strata_vars:\n",
    "            strata_vars = strata_vars.copy()\n",
    "            strata_vars.remove(group_var_name)\n",
    "\n",
    "        result_df_by_group = self.evaluate(\n",
    "            df,\n",
    "            strata_vars=strata_vars + [group_var_name],\n",
    "            result_name=result_name,\n",
    "            weight_var=weight_var,\n",
    "            label_var=label_var,\n",
    "            pred_prob_var=pred_prob_var,\n",
    "        )\n",
    "\n",
    "        if compute_group_min_max:\n",
    "            result_df_min_max = self.compute_group_min_max_fn(\n",
    "                result_df_by_group,\n",
    "                group_var_name=group_var_name,\n",
    "                strata_vars=strata_vars,\n",
    "            )\n",
    "            result_df_min_max[group_var_name] = group_overall_name\n",
    "            result_df_by_group = pd.concat([result_df_by_group, result_df_min_max])\n",
    "\n",
    "        result_df_overall = self.evaluate(\n",
    "            df,\n",
    "            strata_vars=strata_vars,\n",
    "            result_name=result_name,\n",
    "            weight_var=weight_var,\n",
    "            label_var=label_var,\n",
    "            pred_prob_var=pred_prob_var,\n",
    "        )\n",
    "\n",
    "        result_df_overall[group_var_name] = group_overall_name\n",
    "\n",
    "        result_df = pd.concat([result_df_by_group, result_df_overall])\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    def compute_group_min_max_fn(\n",
    "        self, df, group_var_name, result_name=\"performance\", strata_vars=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Computes the min and max of metrics across groups\n",
    "        \"\"\"\n",
    "        strata_vars = self.union_lists([\"metric\"], strata_vars)\n",
    "        result = (\n",
    "            df.query(\"~{}.isnull()\".format(group_var_name), engine=\"python\")\n",
    "            .groupby(strata_vars)[[result_name]]\n",
    "            .agg([\"min\", \"max\"])\n",
    "            .reset_index()\n",
    "            .melt(id_vars=strata_vars)\n",
    "            .assign(metric=lambda x: x[\"metric\"].str.cat(x[\"variable_1\"], sep=\"_\"))\n",
    "            .rename(columns={\"value\": result_name})\n",
    "            .drop(columns=[\"variable_0\", \"variable_1\"])\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    def evaluate_by_group(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Deprecated, but keeping around for legacy purposes\n",
    "        \"\"\"\n",
    "        warnings.warn(\"evaluate_by_group is deprecated, use evaluate\")\n",
    "        return self.evaluate(*args, **kwargs)\n",
    "\n",
    "    def get_metric_fns(\n",
    "        self, metrics=None, threshold_metrics=None, thresholds=None, weighted=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Returns a dictionary of metric functions\n",
    "        Arguments\n",
    "            metrics: a list of string identifiers for metrics defined in get_threshold_free_metrics\n",
    "            threshold_metrics: a list of string identifiers for metrics defined in get_threshold_metrics\n",
    "            thresholds: a list of thresholds to evaluate the threshold based metrics at\n",
    "            weighted: whether the threshold metric functions returned should take a sample_weight argument\n",
    "        \"\"\"\n",
    "        threshold_free_metrics = self.get_threshold_free_metrics(metrics=metrics,)\n",
    "        threshold_metrics = self.get_threshold_metrics(\n",
    "            threshold_metrics=threshold_metrics,\n",
    "            thresholds=thresholds,\n",
    "            weighted=weighted,\n",
    "        )\n",
    "        return {**threshold_free_metrics, **threshold_metrics}\n",
    "\n",
    "    def get_default_threshold_free_metrics(self):\n",
    "        \"\"\"\n",
    "        Defines the string identifiers for the default threshold free metrics\n",
    "        \"\"\"\n",
    "        return [\n",
    "            \"auc\",\n",
    "            \"auprc\",\n",
    "            \"loss_bce\",\n",
    "            \"ace_rmse_logistic_log\",\n",
    "            \"ace_abs_logistic_log\",\n",
    "        ]\n",
    "\n",
    "    def get_threshold_free_metrics(self, thresholds=0.075, metrics=None):\n",
    "        \"\"\"\n",
    "        Defines the set of allowable threshold free metric functions\n",
    "        \"\"\"\n",
    "        base_metric_dict = {\n",
    "            \"auc\": try_roc_auc_score,\n",
    "            \"auprc\": average_precision_score,\n",
    "            \"brier\": brier_score_loss,\n",
    "            \"loss_bce\": try_log_loss,\n",
    "            \"ece_q_abs\": lambda *args, **kwargs: expected_calibration_error(\n",
    "                *args, metric_variant=\"abs\", quantile_bins=True, **kwargs\n",
    "            ),\n",
    "            \"ece_q_rmse\": lambda *args, **kwargs: expected_calibration_error(\n",
    "                *args, metric_variant=\"rmse\", quantile_bins=True, **kwargs\n",
    "            ),\n",
    "            \"ece_abs\": lambda *args, **kwargs: expected_calibration_error(\n",
    "                *args, metric_variant=\"abs\", quantile_bins=False, **kwargs\n",
    "            ),\n",
    "            \"ece_rmse\": lambda *args, **kwargs: expected_calibration_error(\n",
    "                *args, metric_variant=\"rmse\", quantile_bins=False, **kwargs\n",
    "            ),\n",
    "            \"ace_abs_logistic_log\": lambda *args, **kwargs: absolute_calibration_error(\n",
    "                *args,\n",
    "                metric_variant=\"abs\",\n",
    "                model_type=\"logistic\",\n",
    "                transform=\"log\",\n",
    "                **kwargs,\n",
    "            ),\n",
    "            \"ace_abs_bin_log\": lambda *args, **kwargs: absolute_calibration_error(\n",
    "                *args, metric_variant=\"abs\", model_type=\"bin\", transform=\"log\", **kwargs\n",
    "            ),\n",
    "            \"ace_rmse_logistic_log\": lambda *args, **kwargs: absolute_calibration_error(\n",
    "                *args,\n",
    "                metric_variant=\"rmse\",\n",
    "                model_type=\"logistic\",\n",
    "                transform=\"log\",\n",
    "                **kwargs,\n",
    "            ),\n",
    "            \"ace_rmse_bin_log\": lambda *args, **kwargs: absolute_calibration_error(\n",
    "                *args,\n",
    "                metric_variant=\"rmse\",\n",
    "                model_type=\"bin\",\n",
    "                transform=\"log\",\n",
    "                **kwargs,\n",
    "            ),\n",
    "            \"ace_signed_logistic_log\": lambda *args, **kwargs: absolute_calibration_error(\n",
    "                *args,\n",
    "                metric_variant=\"signed\",\n",
    "                model_type=\"logistic\",\n",
    "                transform=\"log\",\n",
    "                **kwargs,\n",
    "            ),\n",
    "            \"ace_signed_bin_log\": lambda *args, **kwargs: absolute_calibration_error(\n",
    "                *args,\n",
    "                metric_variant=\"signed\",\n",
    "                model_type=\"bin\",\n",
    "                transform=\"log\",\n",
    "                **kwargs,\n",
    "            ),\n",
    "            \"ace_abs_logistic_none\": lambda *args, **kwargs: absolute_calibration_error(\n",
    "                *args,\n",
    "                metric_variant=\"abs\",\n",
    "                model_type=\"logistic\",\n",
    "                transform=None,\n",
    "                **kwargs,\n",
    "            ),\n",
    "            \"ace_abs_bin_none\": lambda *args, **kwargs: absolute_calibration_error(\n",
    "                *args, metric_variant=\"abs\", model_type=\"bin\", transform=None, **kwargs\n",
    "            ),\n",
    "            \"ace_rmse_logistic_none\": lambda *args, **kwargs: absolute_calibration_error(\n",
    "                *args,\n",
    "                metric_variant=\"rmse\",\n",
    "                model_type=\"logistic\",\n",
    "                transform=None,\n",
    "                **kwargs,\n",
    "            ),\n",
    "            \"ace_rmse_bin_none\": lambda *args, **kwargs: absolute_calibration_error(\n",
    "                *args,\n",
    "                metric_variant=\"rmse\",\n",
    "                model_type=\"bin\",\n",
    "                transform=None,\n",
    "                **kwargs,\n",
    "            ),\n",
    "            \"ace_signed_logistic_none\": lambda *args, **kwargs: absolute_calibration_error(\n",
    "                *args,\n",
    "                metric_variant=\"signed\",\n",
    "                model_type=\"logistic\",\n",
    "                transform=None,\n",
    "                **kwargs,\n",
    "            ),\n",
    "            \"ace_signed_bin_none\": lambda *args, **kwargs: absolute_calibration_error(\n",
    "                *args,\n",
    "                metric_variant=\"signed\",\n",
    "                model_type=\"bin\",\n",
    "                transform=None,\n",
    "                **kwargs,\n",
    "            ),\n",
    "            \"mean_prediction\": lambda *args, **kwargs: mean_prediction(\n",
    "                *args, the_label=None, **kwargs\n",
    "            ),\n",
    "            \"mean_prediction_0\": lambda *args, **kwargs: mean_prediction(\n",
    "                *args, the_label=0, **kwargs\n",
    "            ),\n",
    "            \"mean_prediction_1\": lambda *args, **kwargs: mean_prediction(\n",
    "                *args, the_label=1, **kwargs\n",
    "            ),\n",
    "            \n",
    "            \"observation_rate\": lambda *args, **kwargs: observation_rates_at_points(\n",
    "                    *args, \n",
    "                    #points=[0.075,0.2],\n",
    "                    model_type=\"logistic\",\n",
    "                    transform=\"log\",\n",
    "                    **kwargs\n",
    "                )\n",
    "        }\n",
    "        if metrics is None:\n",
    "            return base_metric_dict\n",
    "        else:\n",
    "            return {\n",
    "                key: base_metric_dict[key]\n",
    "                for key in metrics\n",
    "                if key in base_metric_dict.keys()\n",
    "            }\n",
    "\n",
    "    def get_threshold_metrics(\n",
    "        self,\n",
    "        threshold_metrics=None,\n",
    "        thresholds=[0.01, 0.05, 0.1, 0.2, 0.5],\n",
    "        weighted=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Returns a set of metric functions that are defined with respect to a set of thresholds\n",
    "        \"\"\"\n",
    "        if thresholds is None:\n",
    "            return {}\n",
    "\n",
    "        if threshold_metrics is None:\n",
    "            threshold_metrics = [\n",
    "                \"recall\",\n",
    "                \"precision\",\n",
    "                \"specificity\",\n",
    "               # \"observation_rate\"\n",
    "            ]  # acts as default value\n",
    "\n",
    "        result = {}\n",
    "\n",
    "        if \"recall\" in threshold_metrics:\n",
    "            result[\"recall\"] = {\n",
    "                \"recall_{}\".format(threshold): generate_recall_at_threshold(\n",
    "                    threshold, weighted=weighted\n",
    "                )\n",
    "                for threshold in thresholds\n",
    "            }\n",
    "        if \"precision\" in threshold_metrics:\n",
    "            result[\"precision\"] = {\n",
    "                \"precision_{}\".format(threshold): generate_precision_at_threshold(\n",
    "                    threshold, weighted=weighted\n",
    "                )\n",
    "                for threshold in thresholds\n",
    "            }\n",
    "        if \"specificity\" in threshold_metrics:\n",
    "            result[\"specificity\"] = {\n",
    "                \"specificity_{}\".format(threshold): generate_specificity_at_threshold(\n",
    "                    threshold, weighted=weighted\n",
    "                )\n",
    "                for threshold in thresholds\n",
    "            }\n",
    "    \n",
    "#         points,\n",
    "#         labels,\n",
    "#         pred_probs,\n",
    "#         sample_weight=None,\n",
    "#         model_type=\"logistic\",\n",
    "#         transform=None,\n",
    "#             \"rate_at_thresholds\": lambda *args, **kwargs: observation_rates_at_points(\n",
    "#                 *args,\n",
    "#                 points=[threshold],\n",
    "#                 model_type=\"logistic\",\n",
    "#                 transform=\"log\",\n",
    "#                 **kwargs,\n",
    "#             ),\n",
    "        if len(result) > 0:\n",
    "            return dict(ChainMap(*result.values()))\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "    def clean_result_df(self, df):\n",
    "\n",
    "        return (\n",
    "            df.query(\"(not performance.isnull())\", engine=\"python\")\n",
    "            .query('(not (metric == \"auc\" & (performance < 0.0)))')\n",
    "            .query('(not (metric == \"loss_bce\" & (performance == 1e18)))')\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def union_lists(x=None, y=None):\n",
    "\n",
    "        if x is not None:\n",
    "            assert isinstance(x, list)\n",
    "            if y is None:\n",
    "                return x\n",
    "        if y is not None:\n",
    "            assert isinstance(y, list)\n",
    "            if x is None:\n",
    "                return y\n",
    "\n",
    "        if (x is not None) and (y is not None):\n",
    "            return list(set(x) | set(y))\n",
    "\n",
    "    def bootstrap_evaluate(\n",
    "        self,\n",
    "        df,\n",
    "        n_boot=1000,\n",
    "        strata_vars_eval=None,\n",
    "        strata_vars_boot=None,\n",
    "        strata_var_replicate=None,\n",
    "        replicate_aggregation_mode=None,\n",
    "        strata_var_experiment=None,\n",
    "        baseline_experiment_name=None,\n",
    "        strata_var_group=None,\n",
    "        compute_overall=False,\n",
    "        group_overall_name=\"overall\",\n",
    "        compute_group_min_max=False,\n",
    "        result_name=\"performance\",\n",
    "        weight_var=None,\n",
    "        label_var=\"labels\",\n",
    "        pred_prob_var=\"pred_probs\",\n",
    "        patient_id_var=\"person_id\",\n",
    "        n_jobs=None,\n",
    "        verbose=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "            df: A dataframe to evaluate\n",
    "            n_boot: The number of bootstrap iterations\n",
    "            stata_vars_eval: The variables for perform stratified evaluation on\n",
    "            strata_vars_boot: The variables to stratify the bootstrap sampling on\n",
    "            strata_vars_replicate: A variable designating replicates\n",
    "            replicate_aggregation_mode: None or 'mean'\n",
    "            strata_var_experiment: The variable designating experimental condition column\n",
    "            baseline_experiment_name: An element of strata_var_experiment column designating a baseline experiment\n",
    "            strata_var_group: The variable designating a group\n",
    "            compute_overall: If true, computes overall metrics without stratifying by group\n",
    "            compute_group_min_max: If true, computes min and max metrics without stratifying by group\n",
    "            result_name: The name of the returned metrics in the result dataframe\n",
    "            weight_var: The variable designating sample weights\n",
    "            label_var: The variable designating the outcome variable\n",
    "            pred_probs_var: The variable designating the predicted score\n",
    "            n_jobs: If None, runs bootstrap iterations serially. Otherwise, specifies the number of jobs for joblib parallelization. -1 uses all cores\n",
    "        \"\"\"\n",
    "\n",
    "        def compute_bootstrap(i=None, verbose=False):\n",
    "            if verbose:\n",
    "                print(f\"Bootstrap iteration: {i}\")\n",
    "            cohort_boot = (\n",
    "                df[[patient_id_var] + strata_vars_boot]\n",
    "                .drop_duplicates()\n",
    "                .groupby(strata_vars_boot)\n",
    "                .sample(frac=1.0, replace=True)\n",
    "            )\n",
    "\n",
    "            df_boot = df.merge(cohort_boot)\n",
    "            if compute_overall or compute_group_min_max:\n",
    "                return self.get_result_df(\n",
    "                    df=df_boot,\n",
    "                    strata_vars=strata_vars_eval,\n",
    "                    group_var_name=strata_var_group,\n",
    "                    weight_var=weight_var,\n",
    "                    compute_group_min_max=compute_group_min_max,\n",
    "                    group_overall_name=group_overall_name,\n",
    "                )\n",
    "            else:\n",
    "                return self.evaluate(\n",
    "                    df=df_boot,\n",
    "                    strata_vars=strata_vars_eval,\n",
    "                    weight_var=weight_var,\n",
    "                    result_name=result_name,\n",
    "                )\n",
    "\n",
    "        if n_jobs is not None:\n",
    "            result = Parallel(n_jobs=n_jobs)(\n",
    "                delayed(compute_bootstrap)(i, verbose=verbose) for i in range(n_boot)\n",
    "            )\n",
    "            result_df = (\n",
    "                pd.concat(result, keys=np.arange(len(result)))\n",
    "                .reset_index(level=-1, drop=True)\n",
    "                .rename_axis(\"boot_id\")\n",
    "                .reset_index()\n",
    "            )\n",
    "        else:\n",
    "            result_df_dict = {}\n",
    "            for i in range(n_boot):\n",
    "                result_df_dict[i] = compute_bootstrap(i, verbose=verbose)\n",
    "            result_df = (\n",
    "                pd.concat(result_df_dict)\n",
    "                .reset_index(level=-1, drop=True)\n",
    "                .rename_axis(\"boot_id\")\n",
    "                .reset_index()\n",
    "            )\n",
    "\n",
    "        strata_vars_ci = strata_vars_eval + [\"metric\"]\n",
    "\n",
    "        if strata_var_replicate is not None:\n",
    "            strata_vars_ci.remove(strata_var_replicate)\n",
    "            if replicate_aggregation_mode is None:\n",
    "                pass\n",
    "            elif replicate_aggregation_mode == \"mean\":\n",
    "                result_df = (\n",
    "                    result_df.groupby(strata_vars_ci + [\"boot_id\"])\n",
    "                    .agg(performance=(result_name, \"mean\"))\n",
    "                    .reset_index()\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(\"Invalid aggregation mode\")\n",
    "\n",
    "        ## Aggregates results ##\n",
    "        if (strata_var_experiment is not None) and (\n",
    "            baseline_experiment_name is not None\n",
    "        ):\n",
    "\n",
    "            result_df_baseline = result_df.query(\n",
    "                f\"{strata_var_experiment} == @baseline_experiment_name\"\n",
    "            )\n",
    "            result_df_baseline = result_df_baseline.drop(columns=strata_var_experiment)\n",
    "            result_df_baseline = result_df_baseline.rename(\n",
    "                columns={f\"{result_name}\": f\"{result_name}_baseline\"}\n",
    "            ).reset_index(drop=True)\n",
    "            result_df_merged = result_df.merge(result_df_baseline)\n",
    "            assert result_df.shape[0] == result_df_merged.shape[0]\n",
    "\n",
    "            result_df = result_df_merged\n",
    "            result_df[f\"{result_name}_delta\"] = (\n",
    "                result_df[f\"{result_name}\"] - result_df[f\"{result_name}_baseline\"]\n",
    "            )\n",
    "\n",
    "            result_df_ci = (\n",
    "                result_df.groupby(strata_vars_ci)\n",
    "                .apply(\n",
    "                    lambda x: pd.DataFrame(\n",
    "                        {\n",
    "                            \"comparator\": np.quantile(\n",
    "                                x[f\"{result_name}\"], [0.025, 0.5, 0.975]\n",
    "                            ),\n",
    "                            \"baseline\": np.quantile(\n",
    "                                x[f\"{result_name}_baseline\"], [0.025, 0.5, 0.975]\n",
    "                            ),\n",
    "                            \"delta\": np.quantile(\n",
    "                                x[f\"{result_name}_delta\"], [0.025, 0.5, 0.975]\n",
    "                            ),\n",
    "                        }\n",
    "                    )\n",
    "                    .rename_axis(\"CI_quantile_95\")\n",
    "                    .rename({i: el for i, el in enumerate([\"lower\", \"mid\", \"upper\"])})\n",
    "                )\n",
    "                .reset_index()\n",
    "            )\n",
    "        else:\n",
    "            # If there are no baselines\n",
    "            result_df_ci = (\n",
    "                result_df.groupby(strata_vars_ci)\n",
    "                .apply(lambda x: np.quantile(x[result_name], [0.025, 0.5, 0.975]))\n",
    "                .rename(result_name)\n",
    "                .reset_index()\n",
    "                .assign(\n",
    "                    CI_lower=lambda x: x[result_name].str[0],\n",
    "                    CI_med=lambda x: x[result_name].str[1],\n",
    "                    CI_upper=lambda x: x[result_name].str[2],\n",
    "                )\n",
    "                .drop(columns=[result_name])\n",
    "            )\n",
    "        return result_df_ci\n",
    "\n",
    "class CalibrationEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluator that computes absolute and relative calibration errors\n",
    "    \"\"\"\n",
    "\n",
    "    def get_calibration_density_df(\n",
    "        self,\n",
    "        labels,\n",
    "        pred_probs,\n",
    "        sample_weight=None,\n",
    "        model_type=\"logistic\",\n",
    "        transform=None,\n",
    "    ):\n",
    "\n",
    "        model = self.init_model(model_type=model_type)\n",
    "\n",
    "        df = pd.DataFrame({\"pred_probs\": pred_probs, \"labels\": labels})\n",
    "        if sample_weight is not None:\n",
    "            df = df.assign(sample_weight=sample_weight)\n",
    "\n",
    "        valid_transforms = [\"log\", \"c_log_log\"]\n",
    "        if transform is None:\n",
    "            df = df.assign(model_input=lambda x: x.pred_probs)\n",
    "            model_input = df.model_input.values.reshape(-1, 1)\n",
    "        elif transform in valid_transforms:\n",
    "            df = df.query(\"(pred_probs > 1e-15) & (pred_probs < (1 - 1e-15))\")\n",
    "            if transform == \"log\":\n",
    "                df = df.assign(model_input=lambda x: np.log(x.pred_probs))\n",
    "            elif transform == \"c_log_log\":\n",
    "                df = df.assign(model_input=lambda x: self.c_log_log(x.pred_probs))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid transform provided\")\n",
    "        model_input = df.model_input.values.reshape(-1, 1)\n",
    "        model.fit(\n",
    "            model_input,\n",
    "            df.labels.values,\n",
    "            sample_weight=df.sample_weight.values\n",
    "            if \"sample_weight\" in df.columns\n",
    "            else None,\n",
    "        )\n",
    "        calibration_density = model.predict_proba(model_input)\n",
    "        if len(calibration_density.shape) > 1:\n",
    "            calibration_density = calibration_density[:, -1]\n",
    "        # df = df.assign(calibration_density=model.predict_proba(model_input)[:, -1])\n",
    "        df = df.assign(calibration_density=calibration_density)\n",
    "        return df, model\n",
    "\n",
    "    def observation_rate_at_point(\n",
    "        self,\n",
    "        labels,\n",
    "        pred_probs,\n",
    "        points=0.075,\n",
    "        sample_weight=None,\n",
    "        model_type=\"logistic\",\n",
    "        transform=None,\n",
    "    ):\n",
    "\n",
    "        df, model = self.get_calibration_density_df(\n",
    "            labels,\n",
    "            pred_probs,\n",
    "            sample_weight=sample_weight,\n",
    "            model_type=model_type,\n",
    "            transform=transform,\n",
    "        )\n",
    "        \n",
    "        valid_transforms = [\"log\", \"c_log_log\"]\n",
    "        \n",
    "        if transform is None:\n",
    "            points = np.array(points).reshape(-1, 1)\n",
    "        elif transform in valid_transforms:\n",
    "            if transform == \"log\":\n",
    "                points = np.array(np.log(points)).reshape(-1, 1)\n",
    "            elif transform == \"c_log_log\":\n",
    "                points = np.array(self.c_log_log(points)).reshape(-1, 1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid transform provided\")\n",
    "        \n",
    "        calibration_density = model.predict_proba(points)\n",
    "        if len(calibration_density.shape) > 1:\n",
    "            calibration_density = calibration_density[:, -1]\n",
    "            \n",
    "        return calibration_density[0]\n",
    "\n",
    "            \n",
    "    def absolute_calibration_error(\n",
    "        self,\n",
    "        labels,\n",
    "        pred_probs,\n",
    "        sample_weight=None,\n",
    "        metric_variant=\"abs\",\n",
    "        model_type=\"logistic\",\n",
    "        transform=None,\n",
    "    ):\n",
    "\n",
    "        df, model = self.get_calibration_density_df(\n",
    "            labels,\n",
    "            pred_probs,\n",
    "            sample_weight=sample_weight,\n",
    "            model_type=model_type,\n",
    "            transform=transform,\n",
    "        )\n",
    "        if \"sample_weight\" in df.columns:\n",
    "            sample_weight = df.sample_weight\n",
    "        else:\n",
    "            sample_weight = None\n",
    "\n",
    "        if metric_variant == \"squared\":\n",
    "            return self.weighted_mean(\n",
    "                (df.calibration_density - df.pred_probs) ** 2,\n",
    "                sample_weight=sample_weight,\n",
    "            )\n",
    "        elif metric_variant == \"rmse\":\n",
    "            return np.sqrt(\n",
    "                self.weighted_mean(\n",
    "                    (df.calibration_density - df.pred_probs) ** 2,\n",
    "                    sample_weight=sample_weight,\n",
    "                )\n",
    "            )\n",
    "        elif metric_variant == \"abs\":\n",
    "            return self.weighted_mean(\n",
    "                np.abs(df.calibration_density - df.pred_probs),\n",
    "                sample_weight=sample_weight,\n",
    "            )\n",
    "        elif metric_variant == \"signed\":\n",
    "            return self.weighted_mean(\n",
    "                df.calibration_density - df.pred_probs, sample_weight=sample_weight\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Invalid option specified for metric\")\n",
    "\n",
    "    def relative_calibration_error(\n",
    "        self,\n",
    "        labels,\n",
    "        pred_probs,\n",
    "        group,\n",
    "        sample_weight=None,\n",
    "        metric_variant=\"abs\",\n",
    "        model_type=\"logistic\",\n",
    "        transform=None,\n",
    "        compute_ace=False,\n",
    "        return_models=False,\n",
    "        return_calibration_density=False,\n",
    "    ):\n",
    "\n",
    "        calibration_density_df_overall, model_overall = self.get_calibration_density_df(\n",
    "            labels,\n",
    "            pred_probs,\n",
    "            sample_weight=sample_weight,\n",
    "            model_type=model_type,\n",
    "            transform=transform,\n",
    "        )\n",
    "\n",
    "        df = pd.DataFrame({\"pred_probs\": pred_probs, \"labels\": labels, \"group\": group})\n",
    "        if sample_weight is not None:\n",
    "            df = df.assign(sample_weight=sample_weight)\n",
    "\n",
    "        ace_dict = {}\n",
    "        rce_dict = {}\n",
    "        model_dict = {}\n",
    "        calibration_density_dict = {}\n",
    "        for group_id, group_df in df.groupby(group):\n",
    "\n",
    "            (\n",
    "                calibration_density_dict[group_id],\n",
    "                model_dict[group_id],\n",
    "            ) = self.get_calibration_density_df(\n",
    "                group_df.labels,\n",
    "                group_df.pred_probs,\n",
    "                sample_weight=group_df.sample_weight\n",
    "                if \"sample_weight\" in group_df.columns\n",
    "                else None,\n",
    "                model_type=model_type,\n",
    "                transform=transform,\n",
    "            )\n",
    "\n",
    "            calib_diff = (\n",
    "                model_dict[group_id].predict_proba(\n",
    "                    calibration_density_dict[group_id].model_input.values.reshape(\n",
    "                        -1, 1\n",
    "                    ),\n",
    "                )[:, -1]\n",
    "                - model_overall.predict_proba(\n",
    "                    calibration_density_dict[group_id].model_input.values.reshape(\n",
    "                        -1, 1\n",
    "                    ),\n",
    "                )[:, -1]\n",
    "            )\n",
    "\n",
    "            group_sample_weight = (\n",
    "                calibration_density_dict[group_id].sample_weight\n",
    "                if \"sample_weight\" in calibration_density_dict[group_id].columns\n",
    "                else None\n",
    "            )\n",
    "            if metric_variant == \"squared\":\n",
    "                rce_dict[group_id] = self.weighted_mean(\n",
    "                    calib_diff ** 2, sample_weight=group_sample_weight\n",
    "                )\n",
    "            elif metric_variant == \"rmse\":\n",
    "                rce_dict[group_id] = np.sqrt(\n",
    "                    self.weighted_mean(\n",
    "                        calib_diff ** 2, sample_weight=group_sample_weight\n",
    "                    )\n",
    "                )\n",
    "            elif metric_variant == \"abs\":\n",
    "                rce_dict[group_id] = self.weighted_mean(\n",
    "                    np.abs(calib_diff), sample_weight=group_sample_weight\n",
    "                )\n",
    "            elif metric_variant == \"signed\":\n",
    "                rce_dict[group_id] = self.weighted_mean(\n",
    "                    calib_diff, sample_weight=group_sample_weight\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(\"Invalid option specified for metric\")\n",
    "\n",
    "            if compute_ace:\n",
    "                if metric_variant == \"squared\":\n",
    "                    ace_dict[group_id] = self.weighted_mean(\n",
    "                        (\n",
    "                            calibration_density_dict[group_id].calibration_density\n",
    "                            - calibration_density_dict[group_id].pred_probs\n",
    "                        )\n",
    "                        ** 2,\n",
    "                        sample_weight=group_sample_weight,\n",
    "                    )\n",
    "                elif metric_variant == \"rmse\":\n",
    "                    ace_dict[group_id] = np.sqrt(\n",
    "                        self.weighted_mean(\n",
    "                            (\n",
    "                                calibration_density_dict[group_id].calibration_density\n",
    "                                - calibration_density_dict[group_id].pred_probs\n",
    "                            )\n",
    "                            ** 2,\n",
    "                            sample_weight=group_sample_weight,\n",
    "                        )\n",
    "                    )\n",
    "                elif metric_variant == \"abs\":\n",
    "                    ace_dict[group_id] = self.weighted_mean(\n",
    "                        np.abs(\n",
    "                            calibration_density_dict[group_id].calibration_density\n",
    "                            - calibration_density_dict[group_id].pred_probs\n",
    "                        ),\n",
    "                        sample_weight=group_sample_weight,\n",
    "                    )\n",
    "                elif metric_variant == \"signed\":\n",
    "                    ace_dict[group_id] = self.weighted_mean(\n",
    "                        calibration_density_dict[group_id].calibration_density\n",
    "                        - calibration_density_dict[group_id].pred_probs,\n",
    "                        sample_weight=group_sample_weight,\n",
    "                    )\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid option specified for metric\")\n",
    "        result_dict = {}\n",
    "        result_dict[\"result\"] = (\n",
    "            pd.DataFrame(rce_dict, index=[\"relative_calibration_error\"])\n",
    "            .transpose()\n",
    "            .rename_axis(\"group\")\n",
    "            .reset_index()\n",
    "        )\n",
    "        if compute_ace:\n",
    "            ace_df = (\n",
    "                pd.DataFrame(ace_dict, index=[\"absolute_calibration_error\"])\n",
    "                .transpose()\n",
    "                .rename_axis(\"group\")\n",
    "                .reset_index()\n",
    "            )\n",
    "            result_dict[\"result\"] = result_dict[\"result\"].merge(ace_df)\n",
    "        if return_models:\n",
    "            result_dict[\"model_dict_group\"] = model_dict\n",
    "            result_dict[\"model_overall\"] = model_overall\n",
    "        if return_calibration_density:\n",
    "            result_dict[\"calibration_density_group\"] = (\n",
    "                pd.concat(calibration_density_dict)\n",
    "                .reset_index(level=-1, drop=True)\n",
    "                .rename_axis(\"group\")\n",
    "                .reset_index()\n",
    "            )\n",
    "            result_dict[\"calibration_density_overall\"] = calibration_density_df_overall\n",
    "        return result_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def c_log_log(x):\n",
    "        return np.log(-np.log(1 - x))\n",
    "\n",
    "    @staticmethod\n",
    "    def weighted_mean(x, sample_weight=None):\n",
    "        if sample_weight is None:\n",
    "            return x.mean()\n",
    "        else:\n",
    "            return np.average(x, weights=sample_weight)\n",
    "\n",
    "    def init_model(self, model_type, **kwargs):\n",
    "        if model_type == \"logistic\":\n",
    "            model = LogisticRegression(\n",
    "                solver=\"lbfgs\", penalty=\"none\", max_iter=10000, **kwargs\n",
    "            )\n",
    "        elif model_type == \"rf\":\n",
    "            model = RandomForestClassifier(**kwargs)\n",
    "        elif model_type == \"bin\":\n",
    "            model = BinningEstimator(**kwargs)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_type not provided\")\n",
    "        return model\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Metrics\n",
    "A metric function takes the following arguments\n",
    "    - labels: A vector of labels\n",
    "    - pred_probs: A vector of predicted probabilities\n",
    "    - sample_weight: A per-sample weight. Default to None\n",
    "    - Optional keyword arguments\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Threshold performance metrics.\n",
    "These metrics define recall, precision, or specificity at a given threshold using the metric function interface\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def recall_at_threshold(labels, pred_probs, sample_weight=None, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Computes recall at a threshold\n",
    "    \"\"\"\n",
    "    return threshold_metric_fn(\n",
    "        labels=labels,\n",
    "        pred_probs=pred_probs,\n",
    "        sample_weight=None,\n",
    "        threshold=threshold,\n",
    "        metric_generator_fn=generate_recall_at_threshold,\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_recall_at_threshold(threshold, weighted=False):\n",
    "    \"\"\"\n",
    "    Returns a lambda function that computes the recall at a provided threshold.\n",
    "    If weights = True, the lambda function takes a third argument for the sample weights\n",
    "    \"\"\"\n",
    "    if not weighted:\n",
    "        return lambda labels, pred_probs: recall_score(\n",
    "            labels, 1.0 * (pred_probs >= threshold)\n",
    "        )\n",
    "    else:\n",
    "        return lambda labels, pred_probs, sample_weight: recall_score(\n",
    "            labels, 1.0 * (pred_probs >= threshold), sample_weight=sample_weight\n",
    "        )\n",
    "    \n",
    "def generate_outcome_rate_at_threshold(threshold):\n",
    "    \"\"\"\n",
    "    Returns a lambda function that computes the recall at a provided threshold.\n",
    "    If weights = True, the lambda function takes a third argument for the sample weights\n",
    "    \"\"\"\n",
    "    return observation_rates_at_points(threshold, *args, **kwargs) \n",
    "\n",
    "# labels, pred_probs: recall_score(\n",
    "#             labels, 1.0 * (pred_probs >= threshold)\n",
    "\n",
    "\n",
    "def precision_at_threshold(labels, pred_probs, sample_weight=None, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Computes precision at a threshold\n",
    "    \"\"\"\n",
    "    return threshold_metric_fn(\n",
    "        labels=labels,\n",
    "        pred_probs=pred_probs,\n",
    "        sample_weight=None,\n",
    "        threshold=threshold,\n",
    "        metric_generator_fn=generate_precision_at_threshold,\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_precision_at_threshold(threshold, weighted=False):\n",
    "    \"\"\"\n",
    "    Returns a lambda function that computes the precision at a provided threshold.\n",
    "    If weights = True, the lambda function takes a third argument for the sample weights\n",
    "    \"\"\"\n",
    "    if not weighted:\n",
    "        return lambda labels, pred_probs: precision_score(\n",
    "            labels, 1.0 * (pred_probs >= threshold), zero_division=0\n",
    "        )\n",
    "    else:\n",
    "        return lambda labels, pred_probs, sample_weight: precision_score(\n",
    "            labels,\n",
    "            1.0 * (pred_probs >= threshold),\n",
    "            zero_division=0,\n",
    "            sample_weight=sample_weight,\n",
    "        )\n",
    "\n",
    "\n",
    "def specificity_at_threshold(labels, pred_probs, sample_weight=None, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Computes specificity at a threshold\n",
    "    \"\"\"\n",
    "    return threshold_metric_fn(\n",
    "        labels=labels,\n",
    "        pred_probs=pred_probs,\n",
    "        sample_weight=None,\n",
    "        threshold=threshold,\n",
    "        metric_generator_fn=generate_specificity_at_threshold,\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_specificity_at_threshold(threshold, weighted=False):\n",
    "    \"\"\"\n",
    "    Returns a lambda function that computes the specificity at a provided threshold.\n",
    "    If weights = True, the lambda function takes a third argument for the sample weights\n",
    "    \"\"\"\n",
    "    if not weighted:\n",
    "        return (\n",
    "            lambda labels, pred_probs: (\n",
    "                (labels == 0) & (labels == (pred_probs >= threshold))\n",
    "            ).sum()\n",
    "            / (labels == 0).sum()\n",
    "            if (labels == 0).sum() > 0\n",
    "            else 0.0\n",
    "        )\n",
    "    else:\n",
    "        return (\n",
    "            lambda labels, pred_probs, sample_weight: (\n",
    "                ((labels == 0) & (labels == (pred_probs >= threshold))) * sample_weight\n",
    "            ).sum()\n",
    "            / ((labels == 0) * sample_weight).sum()\n",
    "            if (labels == 0).sum() > 0\n",
    "            else 0.0\n",
    "        )\n",
    "\n",
    "def threshold_metric_fn(\n",
    "    labels, pred_probs, sample_weight=None, threshold=0.5, metric_generator_fn=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Function that generates threshold metric functions.\n",
    "    Calls a metric_generator_fn for customization\n",
    "    \"\"\"\n",
    "    if metric_generator_fn is None:\n",
    "        raise ValueError(\"metric_generator_fn must not be None\")\n",
    "\n",
    "    metric_fn = metric_generator_fn(\n",
    "        threshold=threshold, weighted=sample_weight is not None\n",
    "    )\n",
    "    if sample_weight is None:\n",
    "        return metric_fn(labels, pred_probs)\n",
    "    else:\n",
    "        return metric_fn(labels, pred_probs, sample_weight=sample_weight)\n",
    "\n",
    "\n",
    "def try_metric_fn(*args, metric_fn=None, default_value=-1, **kwargs):\n",
    "    \"\"\"\n",
    "    Tries to call a metric function, returns default_value if fails\n",
    "    \"\"\"\n",
    "    if metric_fn is None:\n",
    "        raise ValueError(\"Must provide metric_fn\")\n",
    "    try:\n",
    "        return metric_fn(*args, **kwargs)\n",
    "    except ValueError:\n",
    "        warnings.warn(\"Error in metric_fn, filling with default_value\")\n",
    "        return default_value\n",
    "\n",
    "\n",
    "def expected_calibration_error(\n",
    "    labels, pred_probs, num_bins=10, metric_variant=\"abs\", quantile_bins=False\n",
    "):\n",
    "    \"\"\"\n",
    "        Computes the calibration error with a binning estimator over equal sized bins\n",
    "        See http://arxiv.org/abs/1706.04599 and https://arxiv.org/abs/1904.01685.\n",
    "        Does not currently support sample weights\n",
    "    \"\"\"\n",
    "    if metric_variant == \"abs\":\n",
    "        transform_func = np.abs\n",
    "    elif (metric_variant == \"squared\") or (metric_variant == \"rmse\"):\n",
    "        transform_func = np.square\n",
    "    elif metric_variant == \"signed\":\n",
    "        transform_func = identity\n",
    "    else:\n",
    "        raise ValueError(\"provided metric_variant not supported\")\n",
    "\n",
    "    if quantile_bins:\n",
    "        cut_fn = pd.qcut\n",
    "    else:\n",
    "        cut_fn = pd.cut\n",
    "\n",
    "    bin_ids = cut_fn(pred_probs, num_bins, labels=False, retbins=False)\n",
    "    df = pd.DataFrame({\"pred_probs\": pred_probs, \"labels\": labels, \"bin_id\": bin_ids})\n",
    "    ece_df = (\n",
    "        df.groupby(\"bin_id\")\n",
    "        .agg(\n",
    "            pred_probs_mean=(\"pred_probs\", \"mean\"),\n",
    "            labels_mean=(\"labels\", \"mean\"),\n",
    "            bin_size=(\"pred_probs\", \"size\"),\n",
    "        )\n",
    "        .assign(\n",
    "            bin_weight=lambda x: x.bin_size / df.shape[0],\n",
    "            err=lambda x: transform_func(x.pred_probs_mean - x.labels_mean),\n",
    "        )\n",
    "    )\n",
    "    result = np.average(ece_df.err.values, weights=ece_df.bin_weight)\n",
    "    if metric_variant == \"rmse\":\n",
    "        result = np.sqrt(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "def pointwise_expected_calibration_error(\n",
    "    labels,\n",
    "    pred_probs,\n",
    "    sample_weight=None,\n",
    "    num_bins=10,\n",
    "    norm_order=1,\n",
    "    quantile_bins=False,\n",
    "):\n",
    "    \"\"\"\n",
    "        Computes the calibration error with a binning estimator over equal sized bins\n",
    "        Compares individual predicted probabilities with bin estimates\n",
    "        This function implements a version that takes sample weights\n",
    "        For simplicity, bin boundaries are derived from the unweighted sample\n",
    "    \"\"\"\n",
    "    if norm_order == 1:\n",
    "        transform_func = np.abs\n",
    "    elif norm_order == 2:\n",
    "        transform_func = np.square\n",
    "    elif norm_order is None:\n",
    "        transform_func = identity\n",
    "    else:\n",
    "        raise ValueError(\"only norm_order == 1, 2, or None supported\")\n",
    "\n",
    "    if quantile_bins:\n",
    "        cut_fn = pd.qcut\n",
    "    else:\n",
    "        cut_fn = pd.cut\n",
    "\n",
    "    bin_ids = cut_fn(pred_probs, num_bins, labels=False, retbins=False)\n",
    "    data_dict = {\"pred_probs\": pred_probs, \"labels\": labels, \"bin_id\": bin_ids}\n",
    "    if sample_weight is not None:\n",
    "        data_dict[\"sample_weight\"] = sample_weight\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    if sample_weight is None:\n",
    "        ece_df = df.groupby(\"bin_id\").agg(labels_mean=(\"labels\", \"mean\"),).reset_index()\n",
    "        result_df = df.merge(ece_df)\n",
    "        result_df = result_df.assign(\n",
    "            err=lambda x: transform_func(x.pred_probs - x.labels_mean)\n",
    "        )\n",
    "        result = result_df.err.mean()\n",
    "        if norm_order == 2:\n",
    "            result = np.sqrt(result)\n",
    "    else:\n",
    "        ece_df = (\n",
    "            df.groupby(\"bin_id\")\n",
    "            .apply(lambda x: np.average(x.labels, weights=x.sample_weight))\n",
    "            .rename(index=\"labels_mean\")\n",
    "            .reset_index()\n",
    "        )\n",
    "        result_df = df.merge(ece_df)\n",
    "        result_df = result_df.assign(\n",
    "            err=lambda x: transform_func(x.pred_probs - x.labels_mean)\n",
    "        )\n",
    "        result = np.average(\n",
    "            result_df.err.values, weights=result_df.sample_weight.values\n",
    "        )\n",
    "        if norm_order == 2:\n",
    "            result = np.sqrt(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "def identity(x):\n",
    "    \"\"\"\n",
    "    Returns its argument\n",
    "    \"\"\"\n",
    "    return x\n",
    "\n",
    "\n",
    "def mean_prediction(labels, pred_probs, sample_weight=None, the_label=None):\n",
    "    \"\"\"\n",
    "    Computes the mean prediction, optionally conditioning on the_label\n",
    "    \"\"\"\n",
    "    if the_label is not None:\n",
    "        mask = labels == the_label\n",
    "        labels = labels[mask]\n",
    "        pred_probs = pred_probs[mask]\n",
    "        sample_weight = sample_weight[mask] if sample_weight is not None else None\n",
    "\n",
    "    if sample_weight is not None:\n",
    "        return np.average(pred_probs, weights=sample_weight)\n",
    "    else:\n",
    "        return pred_probs.mean()\n",
    "\n",
    "\n",
    "def metric_fairness_ova(\n",
    "    labels,\n",
    "    pred_probs,\n",
    "    group,\n",
    "    the_group,\n",
    "    sample_weight=None,\n",
    "    metric_fn=None,\n",
    "    transform_func=identity,\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes a fairness metric by a comparison of the metric computed over the_group with the metric computed over the marginal distribution\n",
    "    \"\"\"\n",
    "    if metric_fn is None:\n",
    "        raise ValueError(\"metric_fn must be provided\")\n",
    "\n",
    "    if sample_weight is None:\n",
    "        result_group = metric_fn(\n",
    "            labels[group == the_group], pred_probs[group == the_group]\n",
    "        )\n",
    "        result_marginal = metric_fn(labels, pred_probs)\n",
    "        result = transform_func(result_group - result_marginal)\n",
    "    else:\n",
    "        result_group = metric_fn(\n",
    "            labels[group == the_group],\n",
    "            pred_probs[group == the_group],\n",
    "            sample_weight=sample_weight[group == the_group],\n",
    "        )\n",
    "\n",
    "        result_marginal = metric_fn(labels, pred_probs, sample_weight=sample_weight)\n",
    "        result = transform_func(result_group - result_marginal)\n",
    "    return result\n",
    "\n",
    "\n",
    "def roc_auc_ova(*args, **kwargs):\n",
    "    return metric_fairness_ova(*args, metric_fn=roc_auc_score, **kwargs)\n",
    "\n",
    "\n",
    "def average_precision_ova(*args, **kwargs):\n",
    "    return metric_fairness_ova(*args, metric_fn=average_precision_score, **kwargs)\n",
    "\n",
    "\n",
    "def log_loss_ova(*args, **kwargs):\n",
    "    return metric_fairness_ova(*args, metric_fn=log_loss, **kwargs)\n",
    "\n",
    "\n",
    "def brier_ova(*args, **kwargs):\n",
    "    return metric_fairness_ova(*args, metric_fn=brier_score_loss, **kwargs)\n",
    "\n",
    "\n",
    "def mean_prediction_ova(*args, the_label=None, **kwargs):\n",
    "    return metric_fairness_ova(\n",
    "        *args,\n",
    "        metric_fn=lambda *args1, **kwargs1: mean_prediction(\n",
    "            *args1, the_label=the_label, **kwargs1\n",
    "        ),\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "def try_wasserstein_distance(*args, **kwargs):\n",
    "    return try_metric_fn(\n",
    "        *args, metric_fn=scipy.stats.wasserstein_distance, default_value=-1, **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "def try_roc_auc_score(*args, **kwargs):\n",
    "    return try_metric_fn(*args, metric_fn=roc_auc_score, default_value=-1, **kwargs)\n",
    "\n",
    "\n",
    "def try_log_loss(*args, **kwargs):\n",
    "    return try_metric_fn(*args, metric_fn=log_loss, default_value=1e18, **kwargs)\n",
    "\n",
    "\n",
    "def emd_ova(labels, pred_probs, group, the_group, sample_weight=None, the_label=None):\n",
    "    \"\"\"\n",
    "    Computes the earth movers distance between the pred_probs of the_group vs those of the marginal population\n",
    "    Specifying the_label performs the computation stratified on the label\n",
    "    \"\"\"\n",
    "    if the_label is not None:\n",
    "        mask = labels == the_label\n",
    "        labels = labels[mask]\n",
    "        pred_probs = pred_probs[mask]\n",
    "        group = group[mask]\n",
    "        sample_weight = sample_weight[mask] if sample_weight is not None else None\n",
    "\n",
    "    if sample_weight is None:\n",
    "        return try_wasserstein_distance(\n",
    "            u_values=pred_probs[group == the_group], v_values=pred_probs\n",
    "        )\n",
    "    else:\n",
    "        return try_wasserstein_distance(\n",
    "            u_values=pred_probs[group == the_group],\n",
    "            v_values=pred_probs,\n",
    "            u_weights=sample_weight[group == the_group],\n",
    "            v_weights=sample_weight,\n",
    "        )\n",
    "\n",
    "\n",
    "def xauc(\n",
    "    labels,\n",
    "    pred_probs,\n",
    "    group,\n",
    "    the_group,\n",
    "    sample_weight=None,\n",
    "    the_label=1,\n",
    "    exclude_the_group_from_marginal=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the xAUC (http://arxiv.org/abs/1902.05826)\n",
    "        - Computes the AUROC on a dataset composed of \n",
    "            - Data from the intersection of the_group & the_label\n",
    "            - Data from (not the_label), excluding the_group based on exclude_the_group_from_marginal\n",
    "    \"\"\"\n",
    "\n",
    "    other_label = 1 - the_label\n",
    "    mask_group = (group == the_group) & (labels == the_label)\n",
    "    mask_marginal = (\n",
    "        (group != the_group) & (labels == other_label)\n",
    "        if exclude_the_group_from_marginal\n",
    "        else (labels == other_label)\n",
    "    )\n",
    "\n",
    "    if sample_weight is None:\n",
    "        return try_roc_auc_score(\n",
    "            np.concatenate((labels[mask_group], labels[mask_marginal])),\n",
    "            np.concatenate((pred_probs[mask_group], pred_probs[mask_marginal])),\n",
    "        )\n",
    "    else:\n",
    "        return try_roc_auc_score(\n",
    "            np.concatenate((labels[mask_group], labels[mask_marginal])),\n",
    "            np.concatenate((pred_probs[mask_group], pred_probs[mask_marginal])),\n",
    "            sample_weight=np.concatenate(\n",
    "                (sample_weight[mask_group], sample_weight[mask_marginal])\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "# Functions that alias CalibrationEvaluator methods\n",
    "def absolute_calibration_error(*args, **kwargs):\n",
    "    return evaluator.absolute_calibration_error(*args, **kwargs)\n",
    "\n",
    "\n",
    "def relative_calibration_error(*args, **kwargs):\n",
    "    evaluator = CalibrationEvaluator()\n",
    "    return evaluator.relative_calibration_error(*args, **kwargs)\n",
    "\n",
    "def observation_rate_at_point(*args, **kwargs):\n",
    "    evaluator = CalibrationEvaluator()\n",
    "    return evaluator.observation_rate_at_point(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "needed-majority",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-50-d753610720ee>:143: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if group_overall_name in (df[group_var_name].unique()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': <function try_roc_auc_score at 0x7f0948770dc0>, 'observation_rate': <function StandardEvaluator.get_threshold_free_metrics.<locals>.<lambda> at 0x7f094879d430>, 'ace_rmse_logistic_log': <function StandardEvaluator.get_threshold_free_metrics.<locals>.<lambda> at 0x7f0948873160>, 'specificity_0.075': <function generate_specificity_at_threshold.<locals>.<lambda> at 0x7f0948873ee0>, 'specificity_0.2': <function generate_specificity_at_threshold.<locals>.<lambda> at 0x7f0948877790>, 'precision_0.075': <function generate_precision_at_threshold.<locals>.<lambda> at 0x7f0948873f70>, 'precision_0.2': <function generate_precision_at_threshold.<locals>.<lambda> at 0x7f0948873040>, 'recall_0.075': <function generate_recall_at_threshold.<locals>.<lambda> at 0x7f09488734c0>, 'recall_0.2': <function generate_recall_at_threshold.<locals>.<lambda> at 0x7f0948873550>}\n",
      "{'auc': <function try_roc_auc_score at 0x7f0948770dc0>, 'observation_rate': <function StandardEvaluator.get_threshold_free_metrics.<locals>.<lambda> at 0x7f0948697c10>, 'ace_rmse_logistic_log': <function StandardEvaluator.get_threshold_free_metrics.<locals>.<lambda> at 0x7f0948873ee0>, 'specificity_0.075': <function generate_specificity_at_threshold.<locals>.<lambda> at 0x7f0948873940>, 'specificity_0.2': <function generate_specificity_at_threshold.<locals>.<lambda> at 0x7f0948873160>, 'precision_0.075': <function generate_precision_at_threshold.<locals>.<lambda> at 0x7f0948626af0>, 'precision_0.2': <function generate_precision_at_threshold.<locals>.<lambda> at 0x7f094a7ceca0>, 'recall_0.075': <function generate_recall_at_threshold.<locals>.<lambda> at 0x7f094879d430>, 'recall_0.2': <function generate_recall_at_threshold.<locals>.<lambda> at 0x7f0948626a60>}\n"
     ]
    }
   ],
   "source": [
    "standard_evaluator = StandardEvaluator(thresholds = [0.075, 0.2],\n",
    "                                              metrics = ['auc', 'observation_rate',\n",
    "                                                        'ace_rmse_logistic_log'])\n",
    "eval_dict = {'label_var': 'labels',\n",
    "             'pred_prob_var': 'pred_probs',\n",
    "             'weight_var': 'weights',\n",
    "             'group_var_name': 'group'}\n",
    "                                      \n",
    "eval_overall = standard_evaluator.get_result_df(eval_df,\n",
    "                                                    #strata_vars=['model_id', 'fold_id', 'experiment'],\n",
    "                                                    **eval_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "greater-jefferson",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "observation_rate_at_point() got multiple values for argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-8187a5244a09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mgroup_overall_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"overall\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m calibration_df= evaluator.observation_rate_at_point(points, \n\u001b[0m\u001b[1;32m     13\u001b[0m                                                       \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_var\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                                       \u001b[0mpred_probs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred_prob_var\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: observation_rate_at_point() got multiple values for argument 'labels'"
     ]
    }
   ],
   "source": [
    "evaluator = CalibrationEvaluator()\n",
    "\n",
    "df = eval_df\n",
    "points = [0.075, 0.2]\n",
    "weight_var=\"weights\"\n",
    "label_var=\"labels\"\n",
    "pred_prob_var=\"pred_probs\"\n",
    "group_var_name=\"group\"\n",
    "result_name=\"performance\"\n",
    "group_overall_name=\"overall\"\n",
    "\n",
    "calibration_df= evaluator.observation_rate_at_point(points, \n",
    "                                                      labels=df[label_var],\n",
    "                                                      pred_probs=df[pred_prob_var],\n",
    "                                                      sample_weight=df[weight_var],\n",
    "                                                     model_type=\"logistic\", \n",
    "                                                     transform='log')\n",
    "calibration_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:utility]",
   "language": "python",
   "name": "conda-env-utility-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
