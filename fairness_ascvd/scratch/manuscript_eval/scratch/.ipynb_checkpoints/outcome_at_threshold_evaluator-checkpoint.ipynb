{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "personalized-supply",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import prediction_utils.pytorch_utils.metrics as metrics\n",
    "# from prediction_utils.pytorch_utils.metrics import (\n",
    "#     StandardEvaluator,\n",
    "#     CalibrationEvaluator\n",
    "# )\n",
    "\n",
    "from prediction_utils.pytorch_utils.metrics import *\n",
    "from collections import ChainMap\n",
    "\n",
    "\n",
    "class CalibrationEvaluatorNew(CalibrationEvaluator):\n",
    "\n",
    "    def observation_rate_at_point(\n",
    "        self,\n",
    "        point,\n",
    "        labels,\n",
    "        pred_probs,\n",
    "        sample_weight=None,\n",
    "        model_type=\"logistic\",\n",
    "        transform=None,\n",
    "    ):\n",
    "\n",
    "        df, model = self.get_calibration_density_df(\n",
    "            labels,\n",
    "            pred_probs,\n",
    "            sample_weight=sample_weight,\n",
    "            model_type=model_type,\n",
    "            transform=transform,\n",
    "        )\n",
    "        \n",
    "        valid_transforms = [\"log\", \"c_log_log\"]\n",
    "        \n",
    "        if transform is None:\n",
    "            point = np.array(point).reshape(-1, 1)\n",
    "        elif transform in valid_transforms:\n",
    "            if transform == \"log\":\n",
    "                point = np.array(np.log(point)).reshape(-1, 1)\n",
    "            elif transform == \"c_log_log\":\n",
    "                point = np.array(self.c_log_log(point)).reshape(-1, 1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid transform provided\")\n",
    "        \n",
    "        calibration_density = model.predict_proba(point)\n",
    "        if len(calibration_density.shape) > 1:\n",
    "            calibration_density = calibration_density[:, -1]\n",
    "            \n",
    "        return calibration_density[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "sublime-prairie",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prediction_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "nutritional-breeding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'prediction_utils.pytorch_utils.metrics' from '/labs/shahlab/projects/agataf/prediction_utils/prediction_utils/pytorch_utils/metrics.py'>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_utils.pytorch_utils.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "numeric-riding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'prediction_utils.pytorch_utils.metrics' from '/labs/shahlab/projects/agataf/prediction_utils/prediction_utils/pytorch_utils/metrics.py'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "increasing-brush",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardEvaluatorNew(StandardEvaluator):\n",
    "    def get_threshold_metrics(\n",
    "        self,\n",
    "        threshold_metrics=None,\n",
    "        thresholds=[0.01, 0.05, 0.1, 0.2, 0.5],\n",
    "        weighted=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Returns a set of metric functions that are defined with respect to a set of thresholds\n",
    "        \"\"\"\n",
    "        if thresholds is None:\n",
    "            return {}\n",
    "\n",
    "        if threshold_metrics is None:\n",
    "            threshold_metrics = [\n",
    "                \"recall\",\n",
    "                \"precision\",\n",
    "                \"specificity\",\n",
    "            ]  # acts as default value\n",
    "\n",
    "        result = {}\n",
    "\n",
    "        if \"recall\" in threshold_metrics:\n",
    "            result[\"recall\"] = {\n",
    "                \"recall_{}\".format(threshold): generate_recall_at_threshold(\n",
    "                    threshold, weighted=weighted\n",
    "                )\n",
    "                for threshold in thresholds\n",
    "            }\n",
    "        if \"precision\" in threshold_metrics:\n",
    "            result[\"precision\"] = {\n",
    "                \"precision_{}\".format(threshold): generate_precision_at_threshold(\n",
    "                    threshold, weighted=weighted\n",
    "                )\n",
    "                for threshold in thresholds\n",
    "            }\n",
    "        if \"specificity\" in threshold_metrics:\n",
    "            result[\"specificity\"] = {\n",
    "                \"specificity_{}\".format(threshold): generate_specificity_at_threshold(\n",
    "                    threshold, weighted=weighted\n",
    "                )\n",
    "                for threshold in thresholds\n",
    "            }\n",
    "        if \"observation_rate\" in threshold_metrics:\n",
    "            result[\"observation_rate\"] = {\n",
    "                \"observation_rate_{}\".format(threshold): generate_observation_rate_at_threshold(\n",
    "                    threshold, weighted=weighted\n",
    "                )\n",
    "                for threshold in thresholds\n",
    "            }  \n",
    "            \n",
    "        if len(result) > 0:\n",
    "            return dict(ChainMap(*result.values()))\n",
    "        else:\n",
    "            return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "instructional-reflection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_observation_rate_at_threshold(threshold, weighted=False):\n",
    "    \"\"\"\n",
    "    Returns a lambda function that computes the specificity at a provided threshold.\n",
    "    If weights = True, the lambda function takes a third argument for the sample weights\n",
    "    \"\"\"\n",
    "    return (\n",
    "            lambda labels, pred_probs, sample_weight: (\n",
    "             observation_rate_at_point(threshold, labels, pred_probs,\n",
    "                                                      sample_weight,\n",
    "                                                     model_type=\"logistic\", \n",
    "                                                     transform='log')))\n",
    "\n",
    "# def generate_recall_at_threshold(threshold, weighted=False):\n",
    "#     \"\"\"\n",
    "#     Returns a lambda function that computes the recall at a provided threshold.\n",
    "#     If weights = True, the lambda function takes a third argument for the sample weights\n",
    "#     \"\"\"\n",
    "#     if not weighted:\n",
    "#         return lambda labels, pred_probs: recall_score(\n",
    "#             labels, 1.0 * (pred_probs >= threshold)\n",
    "#         )\n",
    "#     else:\n",
    "#         return lambda labels, pred_probs, sample_weight: recall_score(\n",
    "#             labels, 1.0 * (pred_probs >= threshold), sample_weight=sample_weight\n",
    "#         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-coach",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_rate_at_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "decreased-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def observation_rate_at_point(*args, **kwargs):\n",
    "    evaluator = CalibrationEvaluatorNew()\n",
    "    return evaluator.observation_rate_at_point(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "arranged-standing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "grp_label_dict = {1: \"Black women\", 2: \"White women\", 3: \"Black men\", 4: \"White men\"}\n",
    "\n",
    "args = {\n",
    "    \"experiment_name\": \"apr14_erm\",\n",
    "    \"cohort_path\": \"/labs/shahlab/projects/agataf/data/cohorts/pooled_cohorts/cohort/all_cohorts.csv\",\n",
    "    \"base_path\": \"/labs/shahlab/projects/agataf/data/cohorts/pooled_cohorts\",\n",
    "    \"eval_fold\": \"test\",\n",
    "}\n",
    "aggregate_path = os.path.join(\n",
    "    args[\"base_path\"], \"experiments\", args[\"experiment_name\"], \"performance\", \"all\"\n",
    ")\n",
    "\n",
    "preds_path = os.path.join(aggregate_path, \"predictions.csv\")\n",
    "preds = pd.read_csv(preds_path)\n",
    "eval_df = preds.query('phase == \"test\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "damaged-capitol",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0634914643066256"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = CalibrationEvaluatorNew()\n",
    "\n",
    "df = eval_df\n",
    "point = 0.075\n",
    "weight_var=\"weights\"\n",
    "label_var=\"labels\"\n",
    "pred_prob_var=\"pred_probs\"\n",
    "group_var_name=\"group\"\n",
    "result_name=\"performance\"\n",
    "group_overall_name=\"overall\"\n",
    "\n",
    "calibration_df= evaluator.observation_rate_at_point(point, \n",
    "                                                      labels=df[label_var],\n",
    "                                                      pred_probs=df[pred_prob_var],\n",
    "                                                      sample_weight=df[weight_var],\n",
    "                                                     model_type=\"logistic\", \n",
    "                                                     transform='log')\n",
    "calibration_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "permanent-statement",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/labs/shahlab/projects/agataf/prediction_utils/prediction_utils/pytorch_utils/metrics.py:143: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if group_overall_name in (df[group_var_name].unique()):\n"
     ]
    }
   ],
   "source": [
    "standard_evaluator = StandardEvaluatorNew(thresholds = [0.075, 0.2],\n",
    "                                              metrics = ['auc', 'auprc',#'observation_rate',\n",
    "                                                        #'ace_rmse_logistic_log'\n",
    "                                                        ],\n",
    "                                         threshold_metrics=['observation_rate', 'specificity', 'recall'])\n",
    "eval_dict = {'label_var': 'labels',\n",
    "             'pred_prob_var': 'pred_probs',\n",
    "             'weight_var': 'weights',\n",
    "             'group_var_name': 'group'}\n",
    "                                      \n",
    "eval_overall = standard_evaluator.get_result_df(eval_df,\n",
    "                                                    #strata_vars=['model_id', 'fold_id', 'experiment'],\n",
    "                                                    **eval_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "reverse-kentucky",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['auc', 'ace_rmse_logistic_log', 'observation_rate_0.075',\n",
       "       'observation_rate_0.2', 'specificity_0.075', 'specificity_0.2',\n",
       "       'recall_0.075', 'recall_0.2'], dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_overall.metric.unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:utility]",
   "language": "python",
   "name": "conda-env-utility-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
