{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "python pce_inference.py --experiment_name revised_pce\n",
    "python pce_inference.py --experiment_name original_pce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "legal-episode",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/labs/shahlab/projects/agataf/prediction_utils/prediction_utils/pytorch_utils/metrics.py:143: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if group_overall_name in (df[group_var_name].unique()):\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "from prediction_utils.pytorch_utils.metrics import StandardEvaluator, FairOVAEvaluator\n",
    "import utils\n",
    "\n",
    "args = {'experiment_name': 'revised_pce', \n",
    "        'cohort_path': '/labs/shahlab/projects/agataf/data/cohorts/pooled_cohorts/cohort/all_cohorts.csv',\n",
    "        'logging_threshold_metrics': ['sensitivity', 'specificity'],\n",
    "        'logging_thresholds': [0.075, 0.2],\n",
    "        'run_evaluation': True,\n",
    "        'base_path': '/labs/shahlab/projects/agataf/data/cohorts/pooled_cohorts'\n",
    "       }\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--experiment_name', type=str)\n",
    "# parser.add_argument('--base_path', type=str, default='/labs/shahlab/projects/agataf/data/cohorts/pooled_cohorts')\n",
    "# parser.add_argument(\"--cohort_path\", type=str, help=\"path where input cohorts are stored\", required=False,\n",
    "#                    default='/labs/shahlab/projects/agataf/data/cohorts/pooled_cohorts/cohort/all_cohorts.csv')\n",
    "# parser.add_argument('--logging_threshold_metrics', default=['sensitivity', 'specificity'])\n",
    "# parser.add_argument('--logging_thresholds', default=[0.075, 0.2])\n",
    "# parser.add_argument('--run_evaluation', default=False)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# args = copy.deepcopy(args.__dict__)\n",
    "\n",
    "result_path = os.path.join(args['base_path'], 'experiments', args['experiment_name'], 'performance')\n",
    "\n",
    "os.makedirs(result_path, exist_ok=True)\n",
    "os.makedirs(os.path.join(result_path, 'all'), exist_ok=True)\n",
    "\n",
    "cohort = pd.read_csv(args['cohort_path'])\n",
    "cohort = cohort.assign(sysbp = lambda x: x.rxsbp+x.unrxsbp,\n",
    "                       rxbp = lambda x: (x.rxsbp>0).astype(int),\n",
    "                       is_train = lambda x: np.where((x.fold_id != 'eval') & (x.fold_id != \"test\"),\n",
    "                                                         1, 0),\n",
    "                       labels = lambda x: x.ascvd_10yr.astype(int),\n",
    "                       model_type = args['experiment_name'],\n",
    "                       weights = lambda x: utils.get_censoring(x, by_group = True, model_type = 'KM'))\n",
    "\n",
    "if args['experiment_name'] == 'original_pce':\n",
    "    risks = utils.run_pce_model(cohort)\n",
    "elif args['experiment_name'] == 'revised_pce':\n",
    "    risks = utils.run_revised_pce_model(cohort)\n",
    "\n",
    "output_df_eval = (cohort\n",
    "                  .rename(columns={'fold_id': 'phase',\n",
    "                                   'grp': 'group'})\n",
    "                  .join(risks)\n",
    "                  .assign(treat = lambda x: utils.add_ranges(x),\n",
    "                         relative_risk = lambda x: utils.treat_relative_risk(x)\n",
    "                         )\n",
    "                  #.rename(columns={'row_id': 'person_id'})\n",
    "                  .filter(['phase', 'pred_probs', 'labels', 'weights',\n",
    "                           'group', 'model_type', 'person_id', 'treat',\n",
    "                          'relative_risk'])\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "output_df_eval.to_csv(\n",
    "    os.path.join(result_path, 'all', 'predictions.csv'),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "if args['run_evaluation']:\n",
    "\n",
    "    evaluator = StandardEvaluator(threshold_metrics = args['logging_threshold_metrics'],\n",
    "                                  thresholds = args['logging_thresholds'],\n",
    "                                  metrics = ['auc', 'auprc', 'loss_bce', \n",
    "                                             'ace_rmse_logistic_log',\n",
    "                                             'ace_abs_logistic_log']\n",
    "                                 )\n",
    "\n",
    "    eval_general_args = {'df': output_df_eval,\n",
    "                         'label_var': 'labels',\n",
    "                         'pred_prob_var': 'pred_probs',\n",
    "                         'weight_var': 'weights', \n",
    "                         'strata_vars': ['phase'],\n",
    "                         'group_var_name': 'group'}\n",
    "\n",
    "    result_df_overall = evaluator.get_result_df(**eval_general_args)\n",
    "\n",
    "    evaluator = FairOVAEvaluator(threshold_metrics = args['logging_threshold_metrics'],\n",
    "                                 thresholds = args['logging_thresholds'])\n",
    "\n",
    "    eval_fair_args = {'df': output_df_eval,\n",
    "                      'label_var': 'labels',\n",
    "                      'pred_prob_var': 'pred_probs',\n",
    "                      'weight_var': 'weights',\n",
    "                      'group_var_name': 'group',\n",
    "                      'strata_vars': ['phase']}\n",
    "\n",
    "    result_df_group_fair_ova = evaluator.get_result_df(**eval_fair_args)\n",
    "\n",
    "    result_df_overall.to_csv(\n",
    "        os.path.join(result_path, \n",
    "                     'all',\n",
    "                     'standard_evaluation.csv'\n",
    "                     ),\n",
    "        index=False\n",
    "    )\n",
    "    result_df_group_fair_ova.to_csv(\n",
    "        os.path.join(result_path,\n",
    "                     'all',\n",
    "                     'fairness_evaluation.csv'\n",
    "                    ),\n",
    "        index=False\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:utility]",
   "language": "python",
   "name": "conda-env-utility-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
