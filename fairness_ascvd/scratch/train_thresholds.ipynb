{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "import configargparse as argparse\n",
    "\n",
    "from prediction_utils.util import yaml_write\n",
    "from prediction_utils.pytorch_utils.models import TorchModel\n",
    "from prediction_utils.pytorch_utils.lagrangian import MultiLagrangianThresholdRateModel\n",
    "from prediction_utils.pytorch_utils.robustness import GroupDROModel\n",
    "from prediction_utils.pytorch_utils.group_fairness import EqualThresholdRateModel\n",
    "from prediction_utils.pytorch_utils.layers import LinearLayer\n",
    "from prediction_utils.pytorch_utils.metrics import StandardEvaluator, FairOVAEvaluator, CalibrationEvaluator\n",
    "\n",
    "import git\n",
    "repo = git.Repo('.', search_parent_directories=True)\n",
    "os.chdir(repo.working_dir) \n",
    "\n",
    "import train_utils\n",
    "import yaml\n",
    "\n",
    "\n",
    "\n",
    "EXPERIMENT_NAME = 'eq_oddsconstr'\n",
    "config_id = '00'\n",
    "fold_id = '0'\n",
    "BASE_PATH = '/labs/shahlab/projects/agataf/data/cohorts/pooled_cohorts'\n",
    "args = {'experiment_name': EXPERIMENT_NAME,\n",
    "        'cohort_path': '/labs/shahlab/projects/agataf/data/pooled_cohorts/cohort_extraction/all_cohorts.csv',\n",
    "        'base_path': BASE_PATH,\n",
    "        'config_id': config_id,\n",
    "        'fold_id': fold_id,\n",
    "        'print_debug': True,\n",
    "        'save_outputs': True,\n",
    "        'run_evaluation_group_standard': True,\n",
    "        'run_evaluation_group_fair_ova': True,\n",
    "        'save_model_weights': True,\n",
    "        'run_evaluation': True,\n",
    "        #'split_gender': True,\n",
    "        'data_query': ''\n",
    "       }\n",
    "\n",
    "\n",
    "BASE_CONFIG_PATH = os.path.join(args['base_path'], 'experiments', 'basic_config.yaml')\n",
    "\n",
    "\n",
    "RESULT_PATH = os.path.join(args['base_path'], 'experiments', args['experiment_name'], 'performance',\n",
    "                           '.'.join((args['config_id'], 'yaml')), args['fold_id'])\n",
    "LOGGING_PATH = os.path.join(RESULT_PATH, 'training_log.log')\n",
    "\n",
    "args.update({'result_path': RESULT_PATH})\n",
    "\n",
    "##### INITIAL SETUP #####\n",
    "\n",
    "os.makedirs(RESULT_PATH, exist_ok=True)\n",
    "\n",
    "#model_params = yaml.load(open(CONFIG_PATH), Loader=yaml.FullLoader)\n",
    "config_dict = yaml.load(open(BASE_CONFIG_PATH), Loader=yaml.FullLoader)\n",
    "\n",
    "\n",
    "config_dict.update({'logging_path': LOGGING_PATH})\n",
    "update_dict = {\n",
    "    \"threshold_mode\": \"conditional\",\n",
    "    \"thresholds\": [0.075, 0.2],\n",
    "    \"surrogate_scale\": 1.0,\n",
    "    'logging_metrics': ['auc', 'auprc', 'brier', 'loss_bce'],\n",
    "    'data_query': '',\n",
    "    'group_objective_type': 'multiThreshold',\n",
    "    'lambda_group_regularization': 0.01,\n",
    "    'evaluate_by_group': False,\n",
    "    'sparse': False,\n",
    "    'output_dim': 2,\n",
    "    \"num_groups\": 4,\n",
    "    \"sparse\": False,\n",
    "    'num_hidden': 0,\n",
    "    'lr': 1e-4,\n",
    "    'weighted_loss': True\n",
    "}\n",
    "\n",
    "\n",
    "config_dict.update(update_dict)\n",
    "# following https://github.com/som-shahlab/group_robustness_fairness/blob/main/group_robustness_fairness/scripts/tune_baseline_model_starr.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove ##\n",
    "config_dict['num_epochs'] = 5\n",
    "\n",
    "logger = train_utils.logger_setup(config_dict, args)\n",
    "\n",
    "##### DATASET #####\n",
    "data_df = pd.read_csv(args['cohort_path'])\n",
    "\n",
    "if (len(args['data_query']) > 0):\n",
    "    data_df = (data_df\n",
    "               .query(args['data_query'])\n",
    "               .reset_index(drop=True)\n",
    "              )\n",
    "    \n",
    "data_args = train_utils.get_dict_subset(config_dict, ['feature_columns', 'val_fold_id', 'test_fold_id', 'batch_size'])\n",
    "data = train_utils.Dataset(data_df, deg=2, **data_args)\n",
    "\n",
    "# add input dim to dictionary\n",
    "config_dict.update({'input_dim': data.features_dict_uncensored_scaled['train'].shape[1]})\n",
    "\n",
    "# log\n",
    "logger.info(\"Result path: {}\".format(args['result_path']))\n",
    "\n",
    "model, logger = train_utils.model_setup(config_dict, logger, args)\n",
    "\n",
    "result_df = model.train(loaders=data.loaders_dict)['performance']\n",
    "\n",
    "result_df.to_parquet(os.path.join(RESULT_PATH, \"result_df_training.parquet\"), index=False, engine=\"pyarrow\")\n",
    "\n",
    "if args['save_model_weights']:\n",
    "    torch.save(model.model.state_dict(), os.path.join(RESULT_PATH, \"state_dict.pt\"))\n",
    "    \n",
    "if args['run_evaluation']:\n",
    "    logger.info(\"Evaluating model\")\n",
    "\n",
    "    predict_dict = model.predict(data.loaders_dict_predict, \n",
    "                                 phases=['val', 'test'])\n",
    "    \n",
    "    # general evaluation\n",
    "    output_df_eval, result_df_eval = (\n",
    "        predict_dict[\"outputs\"],\n",
    "        predict_dict[\"performance\"]\n",
    "    )\n",
    "\n",
    "    logger.info(result_df_eval)\n",
    "    \n",
    "    output_df_eval = (train_utils.add_ranges(output_df_eval)\n",
    "                      .rename(columns={'row_id': 'person_id'})\n",
    "                      .merge(data_df.filter(['person_id', 'ldlc']), how='inner', on='person_id')\n",
    "                      .assign(relative_risk = lambda x: train_utils.treat_relative_risk(x),\n",
    "                              new_risk = lambda x: x.pred_probs*x.relative_risk\n",
    "                             )\n",
    "                  )\n",
    "    \n",
    "    # Dump evaluation result to disk\n",
    "    result_df_eval.to_parquet(\n",
    "        os.path.join(args['result_path'], \"result_df_training_eval.parquet\"),\n",
    "        index=False,\n",
    "        engine=\"pyarrow\",\n",
    "    )\n",
    "\n",
    "    if args.get('save_outputs'):\n",
    "        output_df_eval.to_parquet(\n",
    "            os.path.join(args['result_path'], \"output_df.parquet\"),\n",
    "            index=False,\n",
    "            engine=\"pyarrow\",\n",
    "        )\n",
    "    \n",
    "    logger = train_utils.evaluation(output_df_eval, args, config_dict, logger)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
